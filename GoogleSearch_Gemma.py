# GoogleSearch_Gemma.py

import requests
import configparser
import base64
from bs4 import BeautifulSoup
import re
from typing import List, Dict, Optional, Tuple
import logging
import torch
from torch.cuda.amp import autocast
# NOTE: transformersì˜ ë¹„ì „ ì˜ì¡´ì„±(torchvision)ë¡œ ì¸í•œ ì„í¬íŠ¸ ì‹¤íŒ¨ë¥¼ í”¼í•˜ê¸° ìœ„í•´
# AutoProcessor / Gemma3ForConditionalGeneration ì„í¬íŠ¸ë¥¼ ëª¨ë“ˆ ë¡œë“œ ì‹œì ì— ìˆ˜í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
# í•„ìš”í•œ ê²½ìš°(ë…ë¦½ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ë‚´ë¶€) ì§€ì—° ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from typing import Optional

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage

# LangGraph ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langgraph.graph import StateGraph, END

# ë¡œê¹… ì„¤ì •
# logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

# =============================================================
# Helper for OSS-20B pipeline (2025-08-14)
# =============================================================

# === ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ í›… (ìˆìœ¼ë©´ ì‚¬ìš©) ===
def _call_langgraph_pipeline(query: str, problem_type: str, language: str) -> Optional[dict]:
    """
    ê¸°ì¡´ ì½”ë“œë² ì´ìŠ¤ì— ì •ì˜ëœ `search_and_reason_for_complex_problem_langgraph`ê°€ ìˆìœ¼ë©´ ì‚¬ìš©.
    ì—†ê±°ë‚˜ ì‹¤íŒ¨í•˜ë©´ None.
    """
    try:
        fn = globals().get('search_and_reason_for_complex_problem_langgraph')
        if callable(fn):
            return fn(
                query=query,
                problem_type=problem_type,
                additional_context=None,
                max_iterations=1,
                language=language
            )
    except Exception as e:
        import traceback
        logging.warning("LangGraph pipeline failed: %s\n%s", e, traceback.format_exc())
    return None

def compose_context_block(snippet: str, plan: str) -> str:
    """LLMì—ê²Œ ì „ë‹¬í•˜ê¸° ì¢‹ì€ ì••ì¶• ì»¨í…ìŠ¤íŠ¸ ë¸”ë¡ì„ êµ¬ì„±í•©ë‹ˆë‹¤."""
    snippet = (snippet or "").strip()
    plan = (plan or "").strip()
    parts = []
    if snippet:
        parts.append("Web Findings (condensed):\n" + snippet)
    if plan:
        parts.append("\nPlan/Method:\n" + plan)
    return "\n\n".join(parts) if parts else "No high-confidence findings."

def get_web_context_for_llm(query: str,
                            problem_type: str = "complex_reasoning_problem",
                            language: str = "ko") -> str:
    """
    [ko] gpt-oss-20bì˜ [[SEARCH: ...]] ìš”ì²­ì— ëŒ€ì‘í•˜ì—¬ ê°„ê²°í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¦¬í„´.
    [en] Compose a compact, drop-in context for the LLM when it asks for web search.

    ì„±ê³µ ì‹œ:
        "Web Findings (condensed): ...\n\nPlan/Method: ..."
    ì‹¤íŒ¨ ì‹œ:
        "Web search unavailable."
    """
    try:
        res = _call_langgraph_pipeline(query, problem_type, language)
        if isinstance(res, dict) and res.get("status") == "success":
            snippet = res.get("best_snippet", "") or res.get("snippet", "") or ""
            plan = res.get("best_plan", "") or res.get("plan", "") or ""
            ctx = compose_context_block(snippet, plan)
            logging.info("[get_web_context_for_llm] Composed context (len=%d)", len(ctx))
            return ctx
        elif isinstance(res, dict):
            # ì‹¤íŒ¨ì´ì§€ë§Œ ìš”ì•½ ì‚¬ìœ ê°€ ìˆì„ ìˆ˜ ìˆìŒ
            summary = res.get("reasoning_summary", "") or "No high-confidence findings."
            logging.info("[get_web_context_for_llm] Fallback summary used.")
            return summary
    except Exception as e:
        import traceback
        logging.error("[get_web_context_for_llm] Failed: %s\n%s", e, traceback.format_exc())
    return "Web search unavailable."

# === [[SEARCH: ...]] íŒ¨í„´ ì¸ì‹ ===
_SEARCH_RE = re.compile(r"\[\[\s*SEARCH\s*:\s*(.*?)\s*\]\]", re.IGNORECASE | re.DOTALL)

def extract_search_request(text: str) -> Optional[str]:
    """
    ëª¨ë¸ ì¶œë ¥ì—ì„œ [[SEARCH: ...]] íŒ¨í„´ì´ ìˆìœ¼ë©´ ì§ˆì˜ ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì—†ìœ¼ë©´ Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not text:
        return None
    m = _SEARCH_RE.search(text)
    if not m:
        return None
    query = (m.group(1) or "").strip()
    # ê³¼ë„í•œ ê°œí–‰/ê³µë°± ì •ë¦¬
    query = re.sub(r"\s+", " ", query)
    logging.info("[extract_search_request] Detected search query: %s", query)
    return query
if not logging.getLogger().hasHandlers():
    logging.basicConfig(level=logging.DEBUG, # ë˜ëŠ” INFO
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                        handlers=[logging.StreamHandler()]) # ëª…ì‹œì ìœ¼ë¡œ í•¸ë“¤ëŸ¬ ì¶”ê°€

# 1. í…ŒìŠ¤íŠ¸
# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì„¤ì •
# def load_model_and_tokenizer():
#     global model, tokenizer
#     model = AutoModel.from_pretrained(
#         'openbmb/MiniCPM-V-2_6-int4', 
#         trust_remote_code=True,
#         # **config
#     )

#     tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6-int4', trust_remote_code=True)

#     model.eval()    
#     return model, tokenizer

# # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì„¤ì •
# def set_model_and_tokenizer(loaded_model=None, loaded_tokenizer=None):
#     global model, tokenizer
#     if loaded_model is None or loaded_tokenizer is None:
#         model, tokenizer = load_model_and_tokenizer()
#     else:
#         model, tokenizer = loaded_model, loaded_tokenizer

# 2. ì±—ë´‡ ì—°ê³„
# ì „ì—­ ë³€ìˆ˜ë¡œ modelê³¼ processor ì„ ì–¸
global model, processor
model = None
processor = None

def set_model_and_processor(loaded_model, loaded_processor):
    global model, processor
    model = loaded_model
    processor = loaded_processor

""" LangChain """

# LangChainì˜ Runnable ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì¡´ Gemma ëª¨ë¸ í˜¸ì¶œì„ ë˜í•‘í•˜ëŠ” í•¨ìˆ˜
def gemma_llm_runner(inputs: dict) -> str:
    """
    LangChain Runnableë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ê¸°ì¡´ Gemma ëª¨ë¸ í˜¸ì¶œ ë¡œì§ì„ ê°ì‹¸ëŠ” í•¨ìˆ˜
    ì…ë ¥ìœ¼ë¡œ dictë¥¼ ë°›ê³  (ì²´ì¸ ì‹¤í–‰ ì‹œ RunnablePassthrough.assignì„ í†µí•´ êµ¬ì„±ë¨),
    ì´ dictëŠ” 'formatted_messages' (List[BaseMessage])ì™€ 'llm_params' (dict) í‚¤ë¥¼ í¬í•¨í•´ì•¼ í•¨.
    LLMì˜ ì‘ë‹µ ë¬¸ìì—´ì„ ë°˜í™˜í•¨.
    """
    global model, processor
    if not model or not processor:
        logging.error("GoogleSearch_Gemma (gemma_llm_runner): Model or processor not set.")
        return "MODEL_OR_PROCESSOR_NOT_SET_ERROR"

    # RunnablePassthrough.assignì„ í†µí•´ ì „ë‹¬ëœ 'formatted_messages'ì™€ 'llm_params'ë¥¼ ê°€ì ¸ì˜´.
    formatted_messages_lc = inputs.get("formatted_messages") # List[BaseMessage] í˜•íƒœ
    llm_params = inputs.get("llm_params", {}) # max_new_tokens ë“±

    if not formatted_messages_lc:
        logging.error("GoogleSearch_Gemma (gemma_llm_runner): 'formatted_messages' not found in input dict.")
        return "FORMATTED_MESSAGES_MISSING_ERROR"
    
    # --- ì¤‘ìš”: List[BaseMessage]ë¥¼ List[Dict[str, str]]ë¡œ ë³€í™˜ ---
    # processor.apply_chat_templateì´ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜.
    # BaseMessageì˜ 'type' ì†ì„±ì„ 'role'ë¡œ ë§¤í•‘í•˜ê³ , 'content' ì†ì„±ì„ ì‚¬ìš©.
    conversation_for_processor = []
    for msg in formatted_messages_lc:
        role = ""
        if isinstance(msg, HumanMessage):
            role = "user"
        elif isinstance(msg, AIMessage):
            role = "assistant"
        elif isinstance(msg, SystemMessage):
            role = "system"
        else: # ê¸°íƒ€ BaseMessage íƒ€ì… (ToolMessage, FunctionMessage ë“±)ì€ í˜„ì¬ ë¡œì§ì—ì„œ ì–´ë–»ê²Œ ì²˜ë¦¬í• ì§€ ì •ì˜ í•„ìš”
            role = "user" # ê¸°ë³¸ê°’ ë˜ëŠ” ì—ëŸ¬ ì²˜ë¦¬ (ToolMessageëŠ” 'tool' ì—­í• , FunctionMessageëŠ” 'function' ì—­í•  ë“±)
            logging.warning(f"GoogleSearch_Gemma (gemma_llm_runner): Unknown message type {type(msg)}, defaulting role to 'user'.")

        # ì—¬ê¸°ì—ì„œ msg.contentì˜ íƒ€ì…ì„ í™•ì¸í•˜ê³ , processorê°€ ê¸°ëŒ€í•˜ëŠ” í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì´ ì¤‘ìš”.
        # Gemma-3 ëª¨ë¸ì˜ chat_templateì€ contentê°€ ë¬¸ìì—´ì´ê±°ë‚˜,
        # [{"type": "text", "text": "..."}] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ì˜ˆìƒ.
        processed_content = ""
        if isinstance(msg.content, str):
            processed_content = msg.content
        elif isinstance(msg.content, list): # ([{"type": "text", "text": "..."}])
            # LangChainì˜ contentê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (ë©€í‹°ëª¨ë‹¬ ë“±), í…ìŠ¤íŠ¸ ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ì—¬ í•©ì¹¨
            text_parts = []
            for part in msg.content:
                if isinstance(part, dict) and part.get("type") == "text":
                    text_parts.append(part.get("text", ""))
                elif isinstance(part, str): # ê°„í˜¹ ë¦¬ìŠ¤íŠ¸ ì•ˆì— ë¬¸ìì—´ì´ ë°”ë¡œ ìˆì„ ìˆ˜ë„ ìˆìŒ
                    text_parts.append(part)
            processed_content = " ".join(text_parts).strip()
            
            # ë§Œì•½ ì‹œê°ì  ì½˜í…ì¸ ê°€ í¬í•¨ëœ ê²½ìš° ê²½ê³  ë¡œê¹…
            visual_content_present = any(isinstance(p, dict) and p.get("type") in ["image", "video"] for p in msg.content)
            if visual_content_present:
                logging.warning(f"GoogleSearch_Gemma (gemma_llm_runner): Visual content detected in LangChain message. It will be ignored as this LLM call is text-only. Message content: {msg.content}")
        else:
            logging.error(f"GoogleSearch_Gemma (gemma_llm_runner): Message content is neither string nor list (type: {type(msg.content)}). Skipping message.")
            continue # ì²˜ë¦¬í•  ìˆ˜ ì—†ëŠ” ë©”ì‹œì§€ëŠ” ê±´ë„ˆë›°ê¸°

        if processed_content: # ë¹ˆ ì½˜í…ì¸ ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ
            content_payload = [
                {"type": "text", "text": processed_content}
            ]
            conversation_for_processor.append({"role": role, "content": content_payload})

    try:
        # conversationì´ ë¹„ì–´ìˆìœ¼ë©´ apply_chat_template ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥
        if not conversation_for_processor:
            logging.warning("GoogleSearch_Gemma (gemma_llm_runner): No valid messages to apply chat template.")
            return "NO_VALID_MESSAGES_ERROR"

        tokenized_inputs = processor.apply_chat_template(
            conversation=conversation_for_processor,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt"
        ).to(model.device)

        input_len = tokenized_inputs["input_ids"].shape[-1]

        max_new_tokens = llm_params.get("max_new_tokens", 150)
        do_sample_param = llm_params.get("do_sample", False)
        temperature = llm_params.get("temperature", 0.7)
        top_p = llm_params.get("top_p")
        top_k = llm_params.get("top_k")

        generation_kwargs = {
            "max_new_tokens": max_new_tokens,
            "do_sample": do_sample_param,
        }
        # do_sample_param ê°’ì— ë”°ë¼ temperature, top_p, top_kë¥¼ ì¡°ê±´ë¶€ë¡œ ì¶”ê°€
        if do_sample_param:
            generation_kwargs["temperature"] = temperature
            if top_p is not None:
                generation_kwargs["top_p"] = top_p
            else:
                generation_kwargs["top_p"] = 0.9 # í•©ë¦¬ì ì¸ ê¸°ë³¸ê°’ ì„¤ì •
            if top_k is not None:
                generation_kwargs["top_k"] = top_k
            else:
                generation_kwargs["top_k"] = 50 # í•©ë¦¬ì ì¸ ê¸°ë³¸ê°’ ì„¤ì •
        else: # do_sample=False ì´ë©´, ìƒ˜í”Œë§ ê´€ë ¨ íŒŒë¼ë¯¸í„°ëŠ” ì „ë‹¬í•˜ì§€ ì•Šê±°ë‚˜ ê¸°ë³¸ê°’ìœ¼ë¡œ
            pass

        with torch.inference_mode():
            generation_output = model.generate(
                **tokenized_inputs,
                **generation_kwargs
            )
            generated_ids = generation_output[0][input_len:]

        decoded_text = processor.decode(generated_ids, skip_special_tokens=True).strip()
       
        if not decoded_text:
            logging.warning("GoogleSearch_Gemma (gemma_llm_runner): LLM produced an empty response.")
        return decoded_text
    except Exception as e:
        # exc_info=True ëŒ€ì‹  logging.exception ì‚¬ìš© ë˜ëŠ” ì˜ˆì™¸ ê°ì²´ ì§ì ‘ ì „ë‹¬
        logging.error(f"GoogleSearch_Gemma (gemma_llm_runner): Error during LLM call - {type(e).__name__}: {e}")
        import traceback
        logging.debug(traceback.format_exc()) # ë””ë²„ê·¸ ë ˆë²¨ë¡œ íŠ¸ë ˆì´ìŠ¤ë°± ì „ì²´ ì¶œë ¥
        return f"LLM_CALL_ERROR: {type(e).__name__} - {str(e)}"
    
# LangChain Runnable ê°ì²´ ìƒì„± (gemma_llm_runner í•¨ìˆ˜ ê¸°ë°˜)
# ì´ ê°ì²´ëŠ” ì—¬ëŸ¬ ì²´ì¸ì—ì„œ ì¬ì‚¬ìš© ê°€ëŠ¥
gemma_runnable = RunnableLambda(gemma_llm_runner)

""" LangChain """

# Error during message handling: local variable 'search_results' referenced before assignment í•´ê²°ì±…ìœ¼ë¡œ search_results ì „ì—­í™”
global search_results
search_results = []

def classify_search_type_langchain(search_query: str, language: str = "en") -> str:
    """
    [LangChain ì ìš© ë²„ì „]
    ì£¼ì–´ì§„ ê²€ìƒ‰ ì¿¼ë¦¬ì˜ ìœ í˜•ì„ LLMì„ ì‚¬ìš©í•´ì„œ ë¶„ë¥˜í•¨.
    - LangChainì˜ ChatPromptTemplate, custom gemma_runnable, StrOutputParserë¥¼ ì‚¬ìš©

    Args:
        search_query (str): ë¶„ë¥˜í•  ê²€ìƒ‰ ì¿¼ë¦¬ ë¬¸ìì—´
        language (str, optional): ê²€ìƒ‰ ì¿¼ë¦¬ì˜ ì–¸ì–´, ê¸°ë³¸ê°’ì€ 'en'

    Returns:
        str: ë¶„ë¥˜ëœ ê²€ìƒ‰ ìœ í˜• (ë¬¸ìì—´)
            (ì˜ˆ: "simple_information_retrieval", "complex_math_problem")
    """
    global model, processor
    if not model or not processor:
        logging.error(f"GoogleSearch_Gemma (classify_search_type_langchain): Model or processor not set for query '{search_query}'.")
        return "simple_information_retrieval" # ëª¨ë¸ ë¯¸ì„¤ì • ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜

    if language == "ko":
        prompt_text = f"""
        ë‹¤ìŒ ì‚¬ìš©ì ê²€ìƒ‰ ìš”ì²­ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ ìœ í˜•ì„ ë¶„ë¥˜í•´ì£¼ì„¸ìš”:
        "{{search_query}}"

        ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” ê²€ìƒ‰ ìœ í˜•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
        - "simple_information_retrieval": ê°„ë‹¨í•œ ì‚¬ì‹¤, ì •ì˜, ìµœì‹  ì •ë³´, íŠ¹ì • ê°œì²´ì— ëŒ€í•œ ì •ë³´ ë“± ì§ì ‘ì ì¸ ì •ë³´ ê²€ìƒ‰ ìš”ì²­ì…ë‹ˆë‹¤. (ì˜ˆ: "ì˜¤ëŠ˜ ì„œìš¸ ë‚ ì”¨", "ì•„ì¸ìŠˆíƒ€ì¸ì€ ëˆ„êµ¬ì¸ê°€", "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”?")
        - "complex_math_problem": ìˆ˜í•™ ê³µì‹ ì ìš©, ë³µì¡í•œ ê³„ì‚°, ìˆ˜í•™ì  ì¦ëª…, íŠ¹ì • ìˆ˜í•™ ì´ë¡  ê²€ìƒ‰ ë“± ë³µì¡í•œ ìˆ˜í•™ ë¬¸ì œ í•´ê²°ê³¼ ê´€ë ¨ëœ ê²€ìƒ‰ ìš”ì²­ì…ë‹ˆë‹¤. (ì˜ˆ: "í˜ë¥´ë§ˆì˜ ë§ˆì§€ë§‰ ì •ë¦¬ ì¦ëª… ê³¼ì •", "ë‚˜ë¹„ì—-ìŠ¤í† í¬ìŠ¤ ë°©ì •ì‹ í’€ì´")
        - "complex_coding_problem": íŠ¹ì • í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì˜ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ ë°©ë²•, ì½”ë“œ ë””ë²„ê¹…, ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©ë²•, ë³µì¡í•œ ì†Œí”„íŠ¸ì›¨ì–´ ì•„í‚¤í…ì²˜ ì„¤ê³„ ë“± ì½”ë”©ê³¼ ê´€ë ¨ëœ ë³µì¡í•œ ë¬¸ì œ í•´ê²° ê²€ìƒ‰ ìš”ì²­ì…ë‹ˆë‹¤. (ì˜ˆ: "íŒŒì´ì¬ìœ¼ë¡œ ì´ë¯¸ì§€ ì¸ì‹ AI ë§Œë“¤ê¸°", "ë¦¬ì•¡íŠ¸ì—ì„œ ìƒíƒœ ê´€ë¦¬ ìµœì í™” ë°©ë²•")
        - "complex_reasoning_problem": ì—¬ëŸ¬ ì •ë³´ë¥¼ ì¢…í•©í•˜ê±°ë‚˜, íŠ¹ì • í˜„ìƒì˜ ì›ì¸ì„ ë¶„ì„í•˜ê±°ë‚˜, ë¯¸ë˜ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•˜ê±°ë‚˜, ë¹„êµ ë¶„ì„í•˜ëŠ” ë“± ê¹Šì´ ìˆëŠ” ì¶”ë¡ ì´ í•„ìš”í•œ ê²€ìƒ‰ ìš”ì²­ì…ë‹ˆë‹¤. (ì˜ˆ: "ì–‘ìì»´í“¨í„°ê°€ ë¯¸ë˜ ì‚¬íšŒì— ë¯¸ì¹  ì˜í–¥ ë¶„ì„", "ê¸°í›„ ë³€í™”ì˜ ì£¼ìš” ì›ì¸ê³¼ í•´ê²° ë°©ì•ˆ ë¹„êµ")

        ìœ„ ì„¤ëª…ê³¼ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ì—¬, ì£¼ì–´ì§„ ê²€ìƒ‰ ìš”ì²­ ë‚´ìš©ì— ê°€ì¥ ì í•©í•œ ìœ í˜• ì´ë¦„ í•˜ë‚˜ë§Œ ì •í™•í•˜ê²Œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
        ë§Œì•½ ë¶„ë¥˜ê°€ ë§¤ìš° ì• ë§¤í•˜ê±°ë‚˜ ìœ„ ìœ í˜•ì— ëª…í™•íˆ ì†í•˜ì§€ ì•ŠëŠ”ë‹¤ê³  íŒë‹¨ë˜ë©´, "simple_information_retrieval"ì„ ë°˜í™˜í•´ì£¼ì„¸ìš”.
        ë‹¤ë¥¸ ì¶”ê°€ ì„¤ëª… ì—†ì´, ìœ í˜• ì´ë¦„ ë¬¸ìì—´ë§Œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
        """
    else: # ì˜ì–´ í”„ë¡¬í”„íŠ¸
        prompt_text = f"""
        Analyze the following user search query content and classify its type:
        "{{search_query}}"

        Possible search type categories are:
        - "simple_information_retrieval": Requests for straightforward factual information, definitions, current events, or information about specific entities. (e.g., "weather in London today", "who is Albert Einstein", "capital of France")
        - "complex_math_problem": Searches related to solving complex mathematical problems, applying formulas, mathematical proofs, or finding specific mathematical theories. (e.g., "proof of Fermat's Last Theorem", "solving Navier-Stokes equations")
        - "complex_coding_problem": Searches for solutions to complex programming tasks, algorithm implementations, code debugging, library usage, or software architecture design. (e.g., "how to build an image recognition AI in Python", "optimizing state management in React")
        - "complex_reasoning_problem": Searches requiring in-depth reasoning, such as analyzing the cause of a phenomenon, predicting future outcomes, synthesizing multiple pieces of information, or comparative analysis. (e.g., "impact of quantum computing on future society", "comparing main causes and solutions for climate change")

        Based on the descriptions and examples above, return only the single most appropriate category name string for the given search query.
        If the classification is very ambiguous or does not clearly fall into any of the above categories, return "simple_information_retrieval".
        You must return only the category name string, with no additional explanation.
        """

    # LangChain ChatPromptTemplate ìƒì„±
    prompt_template = ChatPromptTemplate.from_messages([
        ("human", prompt_text)
    ])

    # LangChain ì²´ì¸ êµ¬ì„± (LCEL - LangChain Expression Language)
    # 1. ì…ë ¥ì„ ë°›ì•„ ({"search_query": ...})
    # 2. RunnablePassthrough.assignì„ ì‚¬ìš©í•˜ì—¬ gemma_runnableì— í•„ìš”í•œ ì…ë ¥ì„ êµ¬ì„±.
    #    - formatted_messages: prompt_templateì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±
    #    - llm_params: ì§ì ‘ ì •ì˜í•œ dict
    # 3. êµ¬ì„±ëœ dictë¥¼ gemma_runnableì— ì „ë‹¬
    # 4. ê²°ê³¼ë¥¼ StrOutputParserë¡œ íŒŒì‹±
    chain = (
        RunnablePassthrough.assign(
            formatted_messages=lambda x: prompt_template.invoke({"search_query": x["search_query"]}).to_messages(),
            llm_params=lambda x: { # LLM ìƒì„± íŒŒë¼ë¯¸í„°ë¥¼ ì—¬ê¸°ì„œ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬
                "max_new_tokens": 40, 
                "do_sample": False, # ë¶„ë¥˜ëŠ” ìƒ˜í”Œë§ ì—†ì´ ì¼ê´€ì„± ìˆê²Œ
                "temperature": 0.1, # ì˜¨ë„ë¥¼ ë‚®ì¶° ì¼ê´€ì„± ê°•í™” (í•„ìš”ì‹œ ì¡°ì •)
                "top_p": None, # do_sample=Falseì´ë©´ ë¬´ì‹œë˜ë¯€ë¡œ Noneìœ¼ë¡œ ë‘ê±°ë‚˜, 1.0ìœ¼ë¡œ ëª…ì‹œ
                "top_k": None  # do_sample=Falseì´ë©´ ë¬´ì‹œë˜ë¯€ë¡œ Noneìœ¼ë¡œ ë‘ê±°ë‚˜, 50ìœ¼ë¡œ ëª…ì‹œ
            }
        )
        | gemma_runnable 
        | StrOutputParser()
    )

    search_type = "simple_information_retrieval" # ê¸°ë³¸ê°’
    llm_raw_output = "" # LLMì˜ ì‹¤ì œ ì¶œë ¥ì„ ì €ì¥í•  ë³€ìˆ˜
    try:
        # ì²´ì¸ ì‹¤í–‰
        response = chain.invoke({
            "search_query": search_query,
            # max_new_tokens, do_sample ë“±ì€ chain ë‚´ë¶€ llm_paramsì—ì„œ ì„¤ì •ë˜ë¯€ë¡œ ì—¬ê¸°ì„œ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬í•˜ì§€ ì•Šì•„ë„ ë¨.
            # í•˜ì§€ë§Œ assign ëŒë‹¤ í•¨ìˆ˜ì—ì„œ x.get()ì„ ì‚¬ìš©í•˜ë¯€ë¡œ, ë§Œì•½ ì—¬ê¸°ì„œ ì „ë‹¬í•˜ë©´ ì˜¤ë²„ë¼ì´ë“œ ë¨.
            # ì—¬ê¸°ì„œëŠ” ì²´ì¸ ë‚´ë¶€ì—ì„œ ì„¤ì •ëœ ê°’ì„ ë”°ë¥´ë„ë¡ í•¨.
        })

        llm_raw_output = response.strip()

        # gemma_llm_runnerì—ì„œ ì—ëŸ¬ ë°œìƒ ì‹œ ë°˜í™˜ë˜ëŠ” ì ‘ë‘ì‚¬ í™•ì¸
        if llm_raw_output.startswith("MODEL_OR_PROCESSOR_NOT_SET_ERROR") or \
           llm_raw_output.startswith("FORMATTED_MESSAGES_MISSING_ERROR") or \
           llm_raw_output.startswith("LLM_CALL_ERROR") or \
           llm_raw_output.startswith("NO_VALID_MESSAGES_ERROR"):
            logging.error(f"GoogleSearch_Gemma (classify_search_type_langchain): LLM runner returned an error - '{llm_raw_output}'. Query: '{search_query}'")
            # ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ë³¸ê°’(simple_information_retrieval) ìœ ì§€
        else:
            valid_types = [
                "simple_information_retrieval",
                "complex_math_problem",
                "complex_coding_problem",
                "complex_reasoning_problem"
            ]
            found_type = next((v_type for v_type in valid_types if v_type in llm_raw_output), None)
            if found_type:
                search_type = found_type
            else:
                logging.warning(f"GoogleSearch_Gemma (classify_search_type_langchain): LLM output '{llm_raw_output}' did not exactly match a valid type. Defaulting. Query: '{search_query}'")
        
        logging.info(f"GoogleSearch_Gemma (classify_search_type_langchain): Classified query '{search_query}' as type: {search_type} (LLM raw: '{llm_raw_output}')")

    except Exception as e:
        logging.error(f"GoogleSearch_Gemma (classify_search_type_langchain): Error during chain execution for query '{search_query}': %s", e)
    
    return search_type

# RAG ì‹œìŠ¤í…œ í´ë˜ìŠ¤ ì •ì˜
class RAGSystem:
    def __init__(self, max_context_length=1000, language=None):
        self.vectorizer = TfidfVectorizer()
        self.vectors = None
        self.documents = []
        self.max_context_length = max_context_length
        self.language = language

    def preprocess_text(self, text: str) -> str:
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ë©”ë‰´, ëª©ì°¨ ë“±ì„ ì œê±°í•˜ê³  í•µì‹¬ ë³¸ë¬¸ë§Œì„ ì¶”ì¶œ)

        # ì¤„ë°”ê¿ˆì„ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• 
        lines = text.split('\n')

        # ì˜ë¯¸ê°€ ìˆëŠ” í…ìŠ¤íŠ¸ ë¼ì¸ë§Œ ë³´ì¡´
        meaningful_lines = []
        for line in lines:
            # ê³µë°±ì„ ì œê±°
            line = line.strip()
            # ì§§ì€ ë¼ì¸, ë©”ë‰´ í•­ëª©(>, :ìœ¼ë¡œ ëë‚˜ëŠ” í…ìŠ¤íŠ¸)ìœ¼ë¡œ ë³´ì´ëŠ” ë¼ì¸ ì œì™¸
            if len(line) > 30 and not line.endswith('>') and ':' not in line:
                meaningful_lines.append(line)

        # ì˜ë¯¸ ìˆëŠ” ë¼ì¸ë“¤ì„ ë‹¤ì‹œ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        preprocessed_text = ' '.join(meaningful_lines)

        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ëŒ€ì²´
        preprocessed_text = re.sub(r'\s+', ' ', preprocessed_text)

        return preprocessed_text

    def add_documents(self, new_documents: List[str]):
        # ë¹ˆ ë¬¸ì„œ ì²´í¬
        if not new_documents or all(not doc.strip() for doc in new_documents):
            logging.warning("Empty documents provided to RAG system")
            # ê¸°ë³¸ ë¬¸ì„œ ì¶”ê°€
            self.documents = ["ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤." if self.language == "ko" else "No search results or unable to process."]
            self.vectors = self.vectorizer.fit_transform(self.documents)
            return

        # ìƒˆë¡œìš´ ë¬¸ì„œë¥¼ ì „ì²˜ë¦¬ í›„
        preprocessed_documents = [self.preprocess_text(doc) for doc in new_documents]

        # ì „ì²˜ë¦¬ëœ ë¬¸ì„œê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        filtered_documents = [doc for doc in preprocessed_documents if doc.strip()]

        if not filtered_documents:
            logging.warning("All documents were empty after preprocessing")
            # ê¸°ë³¸ ë¬¸ì„œ ì¶”ê°€
            self.documents = ["ì „ì²˜ë¦¬ í›„ ëª¨ë“  ë¬¸ì„œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤." if self.language == "ko" else "All documents were empty after preprocessing."]
        else:
            self.documents.extend(filtered_documents)

        # stop_words ì˜µì…˜ì„ Noneìœ¼ë¡œ ì„¤ì •í•´ ëª¨ë“  ë‹¨ì–´ í¬í•¨
        self.vectorizer = TfidfVectorizer(stop_words=None, min_df=1)
        try:
            if self.documents: # ë¬¸ì„œê°€ ìˆì–´ì•¼ë§Œ fit_transform ê°€ëŠ¥
                self.vectors = self.vectorizer.fit_transform(self.documents)
                logging.info(f"Vectorized {len(self.documents)} documents with vocabulary size {len(self.vectorizer.vocabulary_)}")
            else: # ë¬¸ì„œê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° (ì˜ˆ: ì´ˆê¸°í™” ì§í›„ ë˜ëŠ” add_documentsì— ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì „ë‹¬)
                logging.warning("RAGSystem: No documents to vectorize.")
                self.vectors = None
        except Exception as e:
            logging.error(f"RAGSystem: Error in vectorization: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ë¬¸ì„œì™€ ë²¡í„° ì„¤ì •
            self.documents = ["ë²¡í„°í™” ì˜¤ë¥˜ ë°œìƒ." if self.language == "ko" else "Vectorization error occurred."]
            self.vectorizer = TfidfVectorizer(stop_words=None, min_df=1) # vectorizerëŠ” ì´ˆê¸°í™”
            if self.documents: # ë‹¤ì‹œ ì‹œë„
                self.vectors = self.vectorizer.fit_transform(self.documents)
            else:
                self.vectors = None

    def get_relevant_chunks(self, query, n=3) -> List[str]:
        if not self.documents or self.vectors is None or self.vectors.shape[0] == 0: # ë²¡í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš°
            logging.warning("RAGSystem: No documents or vectors available to get relevant chunks.")
            return []
        try:
            # ì¿¼ë¦¬ì™€ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì²­í¬ë¥¼ ì„ íƒ
            query_vector = self.vectorizer.transform([query])
            similarities = cosine_similarity(query_vector, self.vectors)[0]
            top_indices = similarities.argsort()[-n:][::-1]
            return [self.documents[i] for i in top_indices if i < len(self.documents)]
        except Exception as e:
            logging.error(f"RAGSystem: Error getting relevant chunks for query '{query}': {e}")
            return []

    def create_prompt(self, query, relevant_chunks, language="en"):
        # ê´€ë ¨ ì²­í¬ë¥¼ ì‚¬ìš©í•´ì„œ í”„ë¡¬í”„íŠ¸ ìƒì„± (í™˜ê° ìµœì†Œí™” ì§€ì¹¨ í¬í•¨)
        context = " ".join(relevant_chunks)
        if language == "ko":
            prompt = (
                f"Query: {query}\n\n"
                f"Context: {context}\n\n"
                "Instructions (very important):\n"
                "- ë‹µë³€ì€ ë°˜ë“œì‹œ ìœ„ Contextì— ê·¼ê±°í•œ ì‚¬ì‹¤ë§Œ ë‹¨ì •ì ìœ¼ë¡œ ì„œìˆ í•©ë‹ˆë‹¤.\n"
                "- Contextì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì •í•˜ì§€ ë§ê³ , 'ë¬¸ë§¥ì—ì„œ ê·¼ê±° í™•ì¸ ë¶ˆê°€'ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.\n"
                "- ìµœì¢… ì¶œë ¥ì€ ë‹¤ìŒ ë‘ ë¸”ë¡ìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤:\n"
                "  1) í™•ì‹¤: Contextì—ì„œ ì§ì ‘ í™•ì¸ë˜ëŠ” ê²°ë¡ /ì ˆì°¨ë§Œ ê°„ê²°íˆ ì •ë¦¬\n"
                "  2) ë¶ˆí™•ì‹¤/ì¶”ê°€í™•ì¸ í•„ìš”: ë¬¸ë§¥ì— ì—†ê±°ë‚˜ ìƒì¶©í•˜ëŠ” ë¶€ë¶„(ìˆë‹¤ë©´)ê³¼ ë‹¤ìŒ íƒìƒ‰ í‚¤ì›Œë¦¬ìŠ¤íŠ¸ 2-3ê°œ\n"
                "- ë¶ˆí•„ìš”í•œ ë°°ê²½ì§€ì‹ì´ë‚˜ ì¼ë°˜ ìƒì‹ì€ ë„£ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n"
                "Answer:"
            )
        else:
            prompt = (
                f"Query: {query}\n\n"
                f"Context: {context}\n\n"
                "Instructions (very important):\n"
                "- Answer only based on the information in the Context.\n"
                "- If information is not in the Context, indicate 'No information found in context'.\n"
                "- The final output should be structured as follows:\n"
                "  1) Certain: Concisely summarize conclusions/procedures directly from the Context\n"
                "  2) Uncertain/Additional Confirmation Needed: Mention any conflicting or missing information (if any) and 2-3 follow-up search keywords\n"
                "- Do not include unnecessary background knowledge or general facts.\n\n"
                "Answer:"
            )

        # ê°„ë‹¨í•œ í† í° ì¹´ìš´íŒ… í•¨ìˆ˜
        def count_tokens(text):
            # ë§¤ìš° ëŒ€ëµì ì¸ ì¶”ì •ì¹˜. ì‹¤ì œ í† í°í™”ëŠ” ë” ë³µì¡í•¨.
            return len(re.findall(r'\w+', text))

        # í”„ë¡¬í”„íŠ¸ê°€ ì¼ì • ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ë©´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¤„ì„.
        while count_tokens(prompt) > self.max_context_length and len(context) > 0: # contextê°€ ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ ì¤„ì„
            content_len_to_remove = max(1, int(len(context) * 0.05)) # ìµœì†Œ 1ê¸€ìëŠ” ì œê±°í•˜ë„ë¡ ë³´ì¥
            context = context[:-content_len_to_remove]  # (ì»¨í…ìŠ¤íŠ¸ì˜ 5%ë¥¼ ì œê±°)
            prompt = f"Query: {query}\n\nContext: {context}\n\nAnswer:"
        return prompt
    

""" === MCP ìœ ì‚¬ ì¶”ë¡  ë¡œì§ í•¨ìˆ˜ë“¤ (2025.04.03) === """

def evaluate_relevance(problem: str, search_snippet: str, language="en") -> Tuple[bool, str, int]:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ìŠ¤ë‹ˆí«ì´ ë¬¸ì œ í•´ê²°ì— ê´€ë ¨ì„±ì´ ìˆëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤.
    Returns: (ê´€ë ¨ì„± ì—¬ë¶€, ì„¤ëª…, ê´€ë ¨ì„± ì ìˆ˜ 0-10)
    """
    global model, processor
    if not model or not processor:
        logging.error("GoogleSearch_Gemma (evaluate_relevance): Model or processor not set.")
        return False, "Model not available", 0

    if language == "ko":
        prompt = f"""
        ë¬¸ì œ: "{problem}"

        ê²€ìƒ‰ëœ ì •ë³´ ì¡°ê°: "{search_snippet}"

        ì´ ì •ë³´ ì¡°ê°ì´ ìœ„ 'ë¬¸ì œ'ë¥¼ í•´ê²°í•˜ëŠ” ë° ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ì´ ìˆê³  ìœ ìš©í•œì§€ í‰ê°€í•´ì£¼ì„¸ìš”.
        1. ê´€ë ¨ì„± ì—¬ë¶€ (ì˜ˆ/ì•„ë‹ˆì˜¤ ë§Œ ëŒ€ë‹µ)
        2. ì´ìœ  (ê°„ëµíˆ ì„¤ëª…)
        3. ê´€ë ¨ì„± ì ìˆ˜ (0ë¶€í„° 10ê¹Œì§€ì˜ ì •ìˆ˜)

        ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ 3ì¤„ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        ê´€ë ¨ì„±: [ì˜ˆ/ì•„ë‹ˆì˜¤]
        ì´ìœ : [ì„¤ëª…]
        ì ìˆ˜: [ìˆ«ì]
        """
    else:
        prompt = f"""
        Problem: "{problem}"

        Search Snippet Found: "{search_snippet}"

        Evaluate if this snippet is directly relevant and useful for solving the 'Problem' above.
        1. Is it relevant? (Yes/No only)
        2. Why or why not? (Brief explanation)
        3. Relevance Score (Integer from 0 to 10)

        Respond ONLY in the following 3-line format:
        Relevant: [Yes/No]
        Reason: [Explanation]
        Score: [Number]
        """
    # messages = [{"role": "user", "content": [{"type": "text", "text": prompt}]}]
    # inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors="pt").to(model.device, dtype=torch.bfloat16)
    # input_len = inputs["input_ids"].shape[-1]
    lc_chain = (
        RunnablePassthrough.assign(
            formatted_messages=lambda x: ChatPromptTemplate.from_messages([("human", x["prompt_text"])]).invoke({}).to_messages(),
            llm_params=lambda x: {"max_new_tokens": 100, "do_sample": False, "temperature": 0.1} # ì˜¨ë„ ë‚®ì¶° ì¼ê´€ì„± í™•ë³´
        )
        | gemma_runnable
        | StrOutputParser()
    )
    analysis = ""
    try:
        analysis = lc_chain.invoke({"prompt_text": prompt})

        # ê²°ê³¼ íŒŒì‹±
        lines = analysis.split('\n')
        if len(lines) < 3: raise ValueError(f"Incorrect LLM response format for relevance: {analysis}")
        relevant_text = lines[0].split(":", 1)[-1].strip().lower() # [-1] to handle missing colon robustly
        is_relevant = "yes" in relevant_text or "ì˜ˆ" in relevant_text
        reason = lines[1].split(":", 1)[-1].strip()
        score_match = re.search(r'\d+', lines[2].split(":", 1)[-1]) # ìˆ«ìë§Œ ì •í™•íˆ íŒŒì‹±
        if not score_match: raise ValueError(f"Score not found in LLM response: {lines[2]}")
        score = int(score_match.group())
        score = max(0, min(10, score)) # ì ìˆ˜ ë²”ìœ„ ë³´ì •

        logging.debug(f"Relevance Eval Result: Relevant={is_relevant}, Score={score}, Reason='{reason}' for snippet: '{search_snippet[:50]}...'")
        return is_relevant, reason, score
    except Exception as e:
        logging.error(f"Error parsing relevance evaluation: {e}\nLLM Response:\n{analysis}")
        return False, "Parsing error", 0


def plan_application(problem: str, relevant_snippet: str, language="en") -> str:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ìŠ¤ë‹ˆí«ì„ ë¬¸ì œì— ì ìš©í•  ê³„íšì„ ì„¸ì›ë‹ˆë‹¤.
    Returns: ì ìš© ê³„íš ë¬¸ìì—´.
    """
    global model, processor
    if not model or not processor:
        logging.error("GoogleSearch_Gemma (plan_application): Model or processor not set.")
        return "Error: Model not available for planning."

    if language == "ko":
        prompt = f"""
        ë¬¸ì œ: "{problem}"

        ê´€ë ¨ ì •ë³´: "{relevant_snippet}"

        ì£¼ì–´ì§„ 'ê´€ë ¨ ì •ë³´'ë¥¼ ì‚¬ìš©í•˜ì—¬ 'ë¬¸ì œ'ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë‹¨ê³„ë³„ ê³„íšì„ ê°„ëµí•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”. í•µì‹¬ ë‹¨ê³„ì— ì§‘ì¤‘í•˜ê³ , ê° ë‹¨ê³„ëŠ” ëª…í™•í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.

        í•´ê²° ê³„íš:
        """
    else:
        prompt = f"""
        Problem: "{problem}"

        Relevant Information: "{relevant_snippet}"

        Briefly outline a step-by-step plan to use the 'Relevant Information' to potentially solve the 'Problem'. Focus on the key steps, making each step clear.

        Solution Plan:
        """
    lc_chain = (
        RunnablePassthrough.assign(
            formatted_messages=lambda x: ChatPromptTemplate.from_messages([("human", x["prompt_text"])]).invoke({}).to_messages(),
            llm_params=lambda x: {"max_new_tokens": 300, "do_sample": True, "temperature": 0.5}
        )
        | gemma_runnable
        | StrOutputParser()
    )
    plan_str = ""
    try:
        plan_str = lc_chain.invoke({"prompt_text": prompt})
        # "í•´ê²° ê³„íš:" ë˜ëŠ” "Solution Plan:" ê°™ì€ ë¨¸ë¦¬ê¸€ ì œê±°
        plan_str = re.sub(r"^(í•´ê²° ê³„íš:|Solution Plan:)\s*", "", plan_str, flags=re.IGNORECASE).strip()
        logging.debug(f"Generated Plan: {plan_str}")
        return plan_str
    except Exception as e:
        logging.error(f"Error during plan generation: {e}\nLLM Response:\n{plan_str}")
        return f"Error generating plan: {e}"

def evaluate_plan(problem: str, plan: str, language="en") -> Tuple[bool, str, int]:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ ê³„íšì˜ ì„±ê³µ ê°€ëŠ¥ì„±ì„ í‰ê°€í•¨.
    Returns: (íƒ€ë‹¹ì„± ì—¬ë¶€, ë¹„í‰/ê°œì„ ì , ì‹ ë¢°ë„ ì ìˆ˜ 0-10)
    """
    global model, processor
    if not model or not processor:
        logging.error("GoogleSearch_Gemma (evaluate_plan): Model or processor not set.")
        return False, "Model not available for plan evaluation", 0

    if language == "ko":
        prompt = f"""
        ë¬¸ì œ: "{problem}"

        ì œì•ˆëœ í•´ê²° ê³„íš:
        "{plan}"

        ì´ ê³„íšì´ ì£¼ì–´ì§„ 'ë¬¸ì œ'ë¥¼ í•´ê²°í•˜ëŠ” ë…¼ë¦¬ì ì´ê³  í•©ë¦¬ì ì¸ ì ‘ê·¼ ë°©ì‹ì¸ì§€ í‰ê°€í•´ì£¼ì„¸ìš”.
        1. ê³„íšì˜ í•©ë¦¬ì„± (ì˜ˆ/ì•„ë‹ˆì˜¤ ë§Œ ëŒ€ë‹µ)
        2. ì ì¬ì  ë¬¸ì œì  ë˜ëŠ” ê°œì„ ì  (ê°„ëµíˆ ì–¸ê¸‰, ì—†ë‹¤ë©´ "ì—†ìŒ")
        3. ê³„íš ì„±ê³µ ì‹ ë¢°ë„ ì ìˆ˜ (0ë¶€í„° 10ê¹Œì§€ì˜ ì •ìˆ˜)

        ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ 3ì¤„ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        í•©ë¦¬ì„±: [ì˜ˆ/ì•„ë‹ˆì˜¤]
        ê°œì„ ì : [ì„¤ëª… ë˜ëŠ” ì—†ìŒ]
        ì‹ ë¢°ë„: [ìˆ«ì]
        """
    else:
        prompt = f"""
        Problem: "{problem}"

        Proposed Plan:
        "{plan}"

        Evaluate if this plan is a logical and sound approach to solving the 'Problem'.
        1. Is the plan sound? (Yes/No only)
        2. Potential issues or improvements? (Briefly mention, or "None")
        3. Confidence Score in Plan Success (Integer from 0 to 10)

        Respond ONLY in the following 3-line format:
        Sound: [Yes/No]
        Critique: [Explanation or None]
        Confidence: [Number]
        """
    lc_chain = (
        RunnablePassthrough.assign(
            formatted_messages=lambda x: ChatPromptTemplate.from_messages([("human", x["prompt_text"])]).invoke({}).to_messages(),
            llm_params=lambda x: {"max_new_tokens": 150, "do_sample": False, "temperature": 0.1}
        )
        | gemma_runnable
        | StrOutputParser()
    )

    analysis = ""
    try:
        analysis = lc_chain.invoke({"prompt_text": prompt})
        # ê²°ê³¼ íŒŒì‹±
        lines = analysis.split('\n')
        if len(lines) < 3:
             raise ValueError(f"Incorrect response format from LLM for plan eval: {analysis}")
        sound_text = lines[0].split(":", 1)[-1].strip().lower()
        is_sound = "yes" in sound_text or "ì˜ˆ" in sound_text
        critique = lines[1].split(":", 1)[-1].strip()
        confidence_match = re.search(r'\d+', lines[2].split(":", 1)[-1])
        if not confidence_match:
             raise ValueError(f"Confidence score not found in LLM response: {lines[2]}")
        confidence = int(confidence_match.group())
        confidence = max(0, min(10, confidence)) # ì ìˆ˜ ë²”ìœ„ ë³´ì •

        logging.debug(f"Plan Eval: Sound={is_sound}, Confidence={confidence}, Critique='{critique}' for plan: '{plan[:50]}...'")
        return is_sound, critique, confidence
    except Exception as e:
        logging.error(f"Error parsing plan evaluation: {e}\nLLM Response:\n{analysis}")
        return False, f"Parsing error: {e}", 0


# í•µì‹¬ ì¶”ë¡  ì‹¤í–‰ í•¨ìˆ˜
# def search_and_reason_for_complex_problem(query: str, problem_type: str, additional_context: Optional[str] = None, max_iterations: int = 2, language="en") -> Optional[str]:
#     """
#     ë³µì¡í•œ ë¬¸ì œì— ëŒ€í•´ ê²€ìƒ‰, ê´€ë ¨ì„± í‰ê°€, ì ìš© ê³„íš, ê³„íš í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
#     Returns: ìµœì¢… ì‘ë‹µ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ ë˜ëŠ” None (ì˜¤ë¥˜ ì‹œ).
#     """
#     global model, processor, search_results
#     if not model or not processor:
#         logging.error("Model or processor not set for search_and_reason_for_complex_problem.")
#         return None

#     logging.info(f"Starting complex search & reasoning for: '{query}' (Type: {problem_type}, Lang: {language})")
#     reasoning_log = []
#     best_snippet = ""
#     best_plan = ""
#     highest_confidence = -1
#     current_search_query = query # ì´ˆê¸° ê²€ìƒ‰ì–´

#     for i in range(max_iterations):
#         reasoning_log.append(f"\nì¶”ë¡  ë‹¨ê³„ {i+1}:")
#         logging.info(f"Reasoning Iteration {i+1}/{max_iterations}")

#         # 1. ê²€ìƒ‰ ìˆ˜í–‰ (recursive_search í™œìš© ë˜ëŠ” ì§ì ‘ google_search)
#         reasoning_log.append(f"- ê²€ìƒ‰ ì‹¤í–‰: '{current_search_query}'")
#         try:
#             # search_contentëŠ” ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ë˜ëŠ” ì „ì²´ í…ìŠ¤íŠ¸ì¼ ìˆ˜ ìˆìŒ
#             search_content, _, _ = recursive_search(current_search_query, additional_context, max_iterations=1, language=language)
#             # recursive_searchê°€ ì‹¤íŒ¨í•˜ê±°ë‚˜ ë‚´ìš©ì„ ë°˜í™˜í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš° ëŒ€ë¹„
#             if not search_content or not isinstance(search_content, str) or len(search_content) < 10:
#                  logging.warning("Recursive search yielded limited content, trying direct search.")
#                  search_results_list = google_search(current_search_query, num_results=5) # ì§ì ‘ ê²€ìƒ‰
#                  search_content = "\n\n".join([res.get('snippet', '') for res in search_results_list if res.get('snippet')])
#                  if not search_content:
#                       reasoning_log.append("- ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ.")
#                       continue # ë‹¤ìŒ ë°˜ë³µ ì‹œë„ (ì¿¼ë¦¬ ê°œì„  ë¡œì§ì´ ìˆë‹¤ë©´)
#             logging.debug(f"  - ê²€ìƒ‰ ê²°ê³¼ ë‚´ìš© (ì¼ë¶€): {search_content[:200]}...")
#         except Exception as e:
#             logging.error(f"Error during search in iteration {i+1}: {e}")
#             reasoning_log.append(f"- ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#             continue # ë‹¤ìŒ ë°˜ë³µ ì‹œë„

#         # 2. RAGë¡œ í›„ë³´ ìŠ¤ë‹ˆí« ì¶”ì¶œ
#         rag = RAGSystem(language=language)
#         # ê²€ìƒ‰ ê²°ê³¼ê°€ ë¬¸ìì—´ì´ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì„œ ì „ë‹¬
#         rag.add_documents([search_content] if isinstance(search_content, str) else search_content)
#         candidate_snippets = rag.get_relevant_chunks(query, n=5) # ìƒìœ„ 5ê°œ í›„ë³´ ì¶”ì¶œ
#         reasoning_log.append(f"- {len(candidate_snippets)}ê°œì˜ í›„ë³´ ì •ë³´ ì¡°ê° ì¶”ì¶œ.")
#         if not candidate_snippets:
#             continue

#         # 3. ê´€ë ¨ì„± í‰ê°€ (ìƒìœ„ 3ê°œ í‰ê°€)
#         evaluated_snippets = []
#         for snippet in candidate_snippets[:3]:
#             if not snippet or len(snippet.strip()) < 10: continue # ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
#             is_relevant, reason, score = evaluate_relevance(query, snippet, language)
#             reasoning_log.append(f"  - ì •ë³´ ì¡°ê° í‰ê°€: ê´€ë ¨ì„±={is_relevant}, ì ìˆ˜={score}, ì´ìœ ='{reason}', ë‚´ìš©='{snippet[:80]}...'")
#             if is_relevant and score >= 5: # ê´€ë ¨ì„± ì„ê³„ê°’ (ì¡°ì • ê°€ëŠ¥)
#                 evaluated_snippets.append({'text': snippet, 'score': score})

#         if not evaluated_snippets:
#             reasoning_log.append("- ê´€ë ¨ì„± ë†’ì€ ì •ë³´ ì¡°ê° ì—†ìŒ.")
#             # TODO: ì—¬ê¸°ì„œ ë‹¤ìŒ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ê°œì„ í•˜ëŠ” ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
#             # ì˜ˆ: LLMì—ê²Œ "ê´€ë ¨ ì •ë³´ ë¶€ì¡± ì´ìœ ({reason}) ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ì–´ '{current_search_query}' ê°œì„  ì œì•ˆ" ìš”ì²­
#             continue

#         # 4. ìµœê³  ìŠ¤ë‹ˆí« ì„ íƒ ë° ê³„íš ìˆ˜ë¦½/í‰ê°€
#         current_best_snippet_info = max(evaluated_snippets, key=lambda x: x['score'])
#         current_snippet = current_best_snippet_info['text']
#         reasoning_log.append(f"- ìµœì  ì •ë³´ ì¡°ê° ì„ íƒ (ì ìˆ˜: {current_best_snippet_info['score']}): '{current_snippet[:80]}...'")

#         plan = plan_application(query, current_snippet, language)
#         if "Error generating plan" in plan:
#              reasoning_log.append("- ê³„íš ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ.")
#              continue
#         reasoning_log.append(f"- ì ìš© ê³„íš ìƒì„±:\n{plan}")

#         is_sound, critique, confidence = evaluate_plan(query, plan, language)
#         reasoning_log.append(f"  - ê³„íš í‰ê°€: í•©ë¦¬ì„±={is_sound}, ì‹ ë¢°ë„={confidence}, ê°œì„ ì ='{critique}'")

#         # 5. ìµœê³  ê³„íš ì—…ë°ì´íŠ¸
#         if is_sound and confidence > highest_confidence:
#             highest_confidence = confidence
#             best_snippet = current_snippet
#             best_plan = plan
#             reasoning_log.append(f"- ìµœê³  ê³„íš ê°±ì‹  (ì‹ ë¢°ë„: {highest_confidence}).")
#             # ì‹ ë¢°ë„ê°€ ë§¤ìš° ë†’ìœ¼ë©´ ì¼ì° ì¢…ë£Œ ê°€ëŠ¥
#             if confidence >= 8:
#                 reasoning_log.append("- ë†’ì€ ì‹ ë¢°ë„ì˜ ê³„íš ë°œê²¬, ì¶”ë¡  ì¢…ë£Œ.")
#                 break

#         # TODO: ê³„íš í‰ê°€ê°€ ë‚˜ì˜ë©´(`critique` í™œìš©) ë‹¤ìŒ ê²€ìƒ‰ ì¿¼ë¦¬(`current_search_query`) ê°œì„  ë¡œì§

#     # 6. ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ë°˜í™˜
#     final_reasoning_summary = "\n".join(reasoning_log)
#     logging.info("Reasoning process finished. Generating final prompt for Raika.")

#     if best_plan and highest_confidence >= 5:
#         # ì„±ê³µì ì¸ ê²½ìš°, ê²€ìƒ‰ëœ í•µì‹¬ ì •ë³´, ê³„íš, ì¶”ë¡  ê³¼ì •ì„ ë°˜í™˜
#         logging.info(f"ComplexSearch: Success for query='{query}'. Confidence: {highest_confidence}")
#         return {
#             "status": "success",
#             "query": query, # ì›ë³¸ ë¬¸ì œ/ì¿¼ë¦¬
#             "best_snippet": best_snippet,
#             "best_plan": best_plan,
#             "reasoning_summary": final_reasoning_summary,
#             "confidence": highest_confidence,
#             "language": language
#         }
#     else:
#         # ë§Œì¡±ìŠ¤ëŸ¬ìš´ ê³„íš ìˆ˜ë¦½ ì‹¤íŒ¨ ì‹œ
#         logging.warning(f"ComplexSearch: Failure or low confidence for query='{query}'. Highest confidence: {highest_confidence}")
#         return {
#             "status": "failure",
#             "query": query,
#             "reasoning_summary": final_reasoning_summary, # ì‹¤íŒ¨í–ˆì§€ë§Œ, ì‹œë„í•œ ë¡œê·¸ëŠ” ì „ë‹¬
#             "message": "Could not formulate a confident plan based on search results." if language == "en" \
#                        else "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í™•ì‹  ìˆëŠ” ê³„íšì„ ì„¸ìš°ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
#             "language": language
#         }

    # â†‘ ì‹ ë²„ì „ ("1.ê²€ìƒ‰ ê²°ê³¼", "2.ìœ ì € í”„ë¡¬í”„íŠ¸", "3.ë¼ì´ì¹´ ì´ˆê¸° ë‹µë³€"ì„ ì¡°í•©í•˜ê¸° ìœ„í•´ì„œëŠ” ìˆœìˆ˜í•œ "ê²€ìƒ‰ ë° ì¶”ë¡ ì˜ ê²°ê³¼ë¬¼"ì´ í•„ìš”)
    # â†“ êµ¬ë²„ì „ (LLMì—ê²Œ ë„˜ê²¨ì£¼ê¸° ìœ„í•œ ë‹¨ìˆœ ìµœì¢… í”„ë¡¬í”„íŠ¸)

    # if best_plan and highest_confidence >= 5: # ìµœì¢… ê³„íš ì±„íƒ ì„ê³„ê°’ (ì¡°ì • ê°€ëŠ¥)
    #     if language == "ko":
    #         final_prompt = f"""
    #         ì›ë˜ ì§ˆë¬¸: "{query}"
    #         {f"ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸: '{additional_context}'" if additional_context else ""}

    #         ì´ ì§ˆë¬¸ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë‚´ê°€ ìƒê°í•œ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ì•„:
    #         {final_reasoning_summary}

    #         ê°€ì¥ ë„ì›€ì´ ë  ê²ƒ ê°™ì€ ì •ë³´ëŠ” ì´ê±°ì•¼:
    #         "{best_snippet}"

    #         ê·¸ë¦¬ê³  ì´ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ëŠ” ë‚´ ê³„íšì€ ì´ë˜:
    #         "{best_plan}"
    #         (ì´ ê³„íšì— ëŒ€í•œ ë‚´ ì‹ ë¢°ë„ëŠ” {highest_confidence}/10 ì •ë„ì•¼!)

    #         ì, ì´ì œ ìœ„ì˜ ë‚´ ìƒê° ê³¼ì •, ì°¾ì€ ì •ë³´, ê·¸ë¦¬ê³  ê³„íšì„ ë°”íƒ•ìœ¼ë¡œ ì›ë˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ 'ë¼ì´ì¹´'ë¡œì„œ ì‘ì„±í•´ì¤˜! ë‚´ ìƒê° ê³¼ì •ì„ ë‹µë³€ì— ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ì£¼ë©´ ì¢‹ê² ì–´. ë„ˆë¬´ ê¸¸ê²Œ ì„¤ëª…í•˜ì§€ ì•Šì•„ë„ ê´œì°®ì•„. í•„ìš”í•˜ë‹¤ë©´ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•´ì¤˜ë„ ì¢‹ì•„!
    #         """
    #     else:
    #         final_prompt = f"""
    #         Original Question: "{query}"
    #         {f"Additional Context: '{additional_context}'" if additional_context else ""}

    #         Here's how I thought about solving this:
    #         {final_reasoning_summary}

    #         The most helpful piece of information I found seems to be:
    #         "{best_snippet}"

    #         And here's my plan to use that information:
    #         "{best_plan}"
    #         (My confidence in this plan is about {highest_confidence}/10!)

    #         Okay, now, using my thought process, the information I found, and the plan above, please formulate the final answer to the original question *as Raika*! Try to weave my reasoning into your response naturally, without being too lengthy. Feel free to explain step-by-step if it makes sense!
    #         """
    # else:
    #     # ë§Œì¡±ìŠ¤ëŸ¬ìš´ ê³„íš ìˆ˜ë¦½ ì‹¤íŒ¨
    #     if language == "ko":
    #         final_prompt = f"""
    #         ì›ë˜ ì§ˆë¬¸: "{query}"
    #         {f"ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸: '{additional_context}'" if additional_context else ""}

    #         ë‚´ê°€ ì´ ì§ˆë¬¸ì„ í•´ê²°í•˜ë ¤ê³  ì´ë ‡ê²Œ ìƒê°í•´ë´¤ì–´:
    #         {final_reasoning_summary}

    #         *ë‚‘ë‚‘...* ê·¸ëŸ°ë° ì°¾ì•„ë‚¸ ì •ë³´ë“¤ë¡œëŠ” í™•ì‹¤í•˜ê²Œ ë¬¸ì œë¥¼ í•´ê²°í•  ì¢‹ì€ ê³„íšì„ ì„¸ìš°ê¸°ê°€ ì–´ë ¤ì› ì–´. ğŸ˜¥ ì •ë³´ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ë‚´ ê³„íšì´ ì¢€ ë¶€ì¡±í–ˆì„ ìˆ˜ë„ ìˆì–´.
    #         ê·¸ë˜ì„œ ì¼ë‹¨ ë‚´ê°€ ì•„ëŠ” ì„ ì—ì„œ ìµœì„ ì„ ë‹¤í•´ ë‹µí•´ë³¼ê²Œ! í•˜ì§€ë§Œ ì™„ë²½í•˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆë‹¤ëŠ” ì ì€ ì•Œì•„ì¤˜!
    #         """
    #     else:
    #         final_prompt = f"""
    #         Original Question: "{query}"
    #         {f"Additional Context: '{additional_context}'" if additional_context else ""}

    #         Here was my thinking process for tackling this:
    #         {final_reasoning_summary}

    #         *Whimpers softly...* Unfortunately, I couldn't come up with a really solid plan to solve this using the information I found. ğŸ˜¥ Maybe the info wasn't quite right, or my plan wasn't good enough.
    #         So, I'll give you my best answer based on what I already know! Just be aware it might not be perfect!
    #         """

    # logging.debug(f"Final prompt generated for Raika:\n{final_prompt[:500]}...")
    # return final_prompt

""" === MCP ìœ ì‚¬ ì¶”ë¡  ë¡œì§ í•¨ìˆ˜ë“¤ (2025.04.03) === """


def generate_search_keywords_langchain(original_query: str, current_query: str, additional_context: Optional[str] = None, language: str = "en", search_history_summary: Optional[str] = None, strict_user_query_only: bool = False) -> str:
    """
    [LangChain ì ìš© ë²„ì „]
    ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ (ì›í•˜ëŠ” ê²°ê³¼ê°€ ì˜¨ì „íˆ ë‚˜ì˜¤ë„ë¡) ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ìƒì„±
    
    Args:
        strict_user_query_only: Trueì¼ ê²½ìš°, search_history_summaryì™€ additional_contextë¥¼ ë¬´ì‹œí•˜ê³ 
                               ì˜¤ë¡œì§€ original_queryë§Œ ê¸°ë°˜ìœ¼ë¡œ í‚¤ì›Œë“œ ìƒì„±
    """
    
    global model, processor
    if not model or not processor:
        logging.error("GoogleSearch_Gemma (generate_search_keywords_langchain): Model or processor not set.")
        return original_query # ëª¨ë¸ ì—†ìœ¼ë©´ ì›ë³¸ ì¿¼ë¦¬ ë°˜í™˜ ë˜ëŠ” ë¹ˆ ë¬¸ìì—´
    
    # strict_user_query_only ëª¨ë“œì¼ ê²½ìš° search_historyì™€ additional_context ë¬´ì‹œ
    if strict_user_query_only:
        logging.info("generate_search_keywords_langchain: strict_user_query_only mode - ignoring search history and additional context")
        search_history_summary = None
        additional_context = None
    
    # ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±
    if language == "ko":
        prompt_text = f"""
        ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸: {{original_query}}
        ì´ì „ ê²€ìƒ‰ ì‹œë„ì—ì„œ ì‚¬ìš©í•œ ê²€ìƒ‰ì–´: {{current_query}}
        ì¶”ê°€ì ì¸ ë§¥ë½ ì •ë³´: {{additional_context_str}}
        {f"ì´ì „ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½: {{search_history_summary}}" if search_history_summary else ""}

        ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸ì— ë” ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì°¾ê¸° ìœ„í•œ **ìƒˆë¡œìš´** êµ¬ê¸€ ê²€ìƒ‰ ì§ˆì˜ 3-4ê°œë¥¼ ì œì•ˆí•˜ì„¸ìš”.
        
        **í•µì‹¬ ì§€ì¹¨ (Query Rewriting Strategy):**
        1. **ë‹¤êµ­ì–´ í™•ì¥**: ì›ë³¸ ì§ˆë¬¸ì´ í•œêµ­ì–´ë¼ë„, (í•œêµ­ ê³ ìœ ì˜ ì»¨í…ì¸ ê°€ ì•„ë‹ˆë¼ë©´) ì •ë³´ëŸ‰ì´ ë§ì€ **ì˜ì–´(English)** ê²€ìƒ‰ì–´ë¥¼ ë°˜ë“œì‹œ 1~2ê°œ í¬í•¨í•˜ì„¸ìš”. (ì˜ˆ: 'ë‚¨ë¯¸ ì˜í™” í¸ì§€' -> 'South American movie letter writing')
        2. **êµ¬ì²´ì  ë¬˜ì‚¬**: "ê°€ê²©/ì •ë³´" ê°™ì€ ë‹¨ìˆœ í‚¤ì›Œë“œë³´ë‹¤ëŠ”, ì§ˆë¬¸ì˜ ë¬˜ì‚¬ì  íŠ¹ì§•(description)ì„ ì‚´ë¦° êµ¬ì²´ì ì¸ êµ¬ë¬¸(phrase)ì„ ì‚¬ìš©í•˜ì„¸ìš”.
        3. **ì—”í‹°í‹° ë³´ì¡´**: ê³ ìœ ëª…ì‚¬, ì—°ë„, íŠ¹ì • í–‰ìœ„ ë“± í•µì‹¬ ì—”í‹°í‹°ëŠ” ìœ ì§€í•˜ë˜, ë™ì˜ì–´ë‚˜ ìœ ì˜ì–´ë¡œ ë³€í˜•í•˜ì—¬ ì‹œë„í•˜ì„¸ìš”.
        4. **í˜•ì‹**: ì§ˆì˜ëŠ” ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•˜ê³ , ê° ì§ˆì˜ëŠ” ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
        5. ë” ì´ìƒ ê°œì„ ëœ ì§ˆì˜ê°€ ì—†ë‹¤ë©´ "ë” ì´ìƒ ì¢‹ì€ í‚¤ì›Œë“œ ì—†ìŒ"ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

        ìƒˆë¡œìš´ ê²€ìƒ‰ ì§ˆì˜:
        """
    else:
        prompt_text = f"""
        Original user request: {{original_query}}
        Search query used in the previous attempt: {{current_query}}
        Additional context: {{additional_context_str}}
        {f"Summary of previous search results: {{search_history_summary}}" if search_history_summary else ""}

        Based on the above, suggest 2-3 **new** Google search queries.
        Guidelines:
        - Preserve salient named entities (brands, model names, places, dates) from the original request.
        - Do not output generic words alone (e.g., price/specs/compare/info); if used, pair them with the salient entities.
        - Make queries domain-agnostic but specific (news, academic, guides, games, books, etc.).
        - Separate by commas, each query 3-8 tokens.
        - If no significantly better queries exist, respond only with "NO_BETTER_KEYWORDS".

        New search queries:
        """

    prompt_template = ChatPromptTemplate.from_messages([
        ("human", prompt_text)
    ])

    chain = (
        RunnablePassthrough.assign(
            formatted_messages=lambda x: prompt_template.invoke({
                "original_query": x["original_query"],
                "current_query": x["current_query"],
                "additional_context_str": x["additional_context_str"],
                "search_history_summary": x.get("search_history_summary") # Noneì¼ ìˆ˜ ìˆìŒ
            }).to_messages(),
            llm_params=lambda x: { # LLM ìƒì„± íŒŒë¼ë¯¸í„°ë¥¼ ì—¬ê¸°ì„œ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬
                "max_new_tokens": x.get("max_new_tokens", 75),
                "do_sample": x.get("do_sample", True),
                "temperature": x.get("temperature", 0.6),
                "top_p": x.get("top_p"),
                "top_k": x.get("top_k")
            }
        )
        | gemma_runnable
        | StrOutputParser()
    )

    keywords_str = original_query # ê¸°ë³¸ê°’: LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì¿¼ë¦¬ë¥¼ í‚¤ì›Œë“œë¡œ ì‚¬ìš©
    llm_raw_output = ""
    try:
        # additional_contextê°€ Noneì¼ ê²½ìš° ë¹ˆ ë¬¸ìì—´ ë˜ëŠ” "ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ" ë“±ìœ¼ë¡œ ì²˜ë¦¬
        context_str = additional_context if additional_context else ("ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ" if language == "ko" else "No additional context provided")
        history_summary_str = search_history_summary if search_history_summary else ("ì´ì „ ê²€ìƒ‰ ê¸°ë¡ ì—†ìŒ" if language == "ko" else "No previous search history")

        response = chain.invoke({
            "original_query": original_query,
            "current_query": current_query,
            "additional_context_str": context_str,
            "search_history_summary": history_summary_str,
            "max_new_tokens": 75, # í‚¤ì›Œë“œ ìƒì„±ì´ë¯€ë¡œ ì ì ˆí•œ ê¸¸ì´
            "do_sample": True,     # ë‹¤ì–‘í•œ í‚¤ì›Œë“œ ìƒì„±ì„ ìœ„í•´ True
            "temperature": 0.6     # ë„ˆë¬´ ë²—ì–´ë‚˜ì§€ ì•ŠëŠ” ì„ ì—ì„œ ì°½ì˜ì„± ë¶€ì—¬
        })
        llm_raw_output = response.strip()

        if llm_raw_output.startswith("MODEL_OR_PROCESSOR_NOT_SET_ERROR") or \
           llm_raw_output.startswith("FORMATTED_MESSAGES_MISSING_ERROR") or \
           llm_raw_output.startswith("LLM_CALL_ERROR"):
            logging.error(f"GoogleSearch_Gemma (generate_search_keywords_langchain): LLM runner returned an error - '{llm_raw_output}'. Query: '{original_query}'")
            # ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ë³¸ê°’(original_query) ìœ ì§€
        elif not llm_raw_output: # LLM ì‘ë‹µì´ ë¹„ì–´ìˆëŠ” ê²½ìš°
            logging.warning(f"GoogleSearch_Gemma (generate_search_keywords_langchain): LLM returned empty keywords for query '{original_query}'. Using original query as fallback.")
        else:
            # [25.11.26 íŒŒì‹± ê°•í™”] LLM ì¶œë ¥ì´ ë²ˆí˜¸ ëª©ë¡ì´ë‚˜ ê°œí–‰ìœ¼ë¡œ êµ¬ë¶„ë  ê²½ìš° ì²˜ë¦¬
            # ì˜ˆ: "1. kw1\n2. kw2" -> "kw1, kw2"
            cleaned_lines = []
            for line in llm_raw_output.split('\n'):
                line = line.strip()
                if not line: continue
                # ë²ˆí˜¸(1., 1)) ë° ë¶ˆë ›(-, *) ì œê±°
                line = re.sub(r'^[\d]+[\.\)]\s*|^[\-\*]\s*', '', line)
                # ì•ë’¤ ë”°ì˜´í‘œ ì œê±°
                line = line.strip('"\'')
                if line:
                    cleaned_lines.append(line)
            
            # ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬ëœ í•­ëª©ë“¤ì„ ì½¤ë§ˆë¡œ ì—°ê²°
            joined_text = ",".join(cleaned_lines)
            
            # ì½¤ë§ˆë¡œ ì¬ë¶„ë¦¬í•˜ì—¬ ê¹”ë”í•œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
            final_kws = [k.strip() for k in joined_text.split(',') if k.strip()]
            
            if final_kws:
                keywords_str = ", ".join(final_kws)
            else:
                keywords_str = llm_raw_output # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‚¬ìš© (fallback)
        
        logging.info(f"GoogleSearch_Gemma (generate_search_keywords_langchain): Generated keywords '{keywords_str}' for query '{original_query}' (LLM raw: '{llm_raw_output}')")
    except Exception as e:
        logging.error(f"GoogleSearch_Gemma (generate_search_keywords_langchain): Error generating keywords for query '{original_query}': %s", e)

    return keywords_str

def recursive_search(initial_query: str, additional_context: Optional[str] = None, max_iterations: int = 3, language="en", *, user_query: Optional[str] = None, user_info_uncertain: bool = False) -> tuple:
    """
    ì¬ê·€ì  ê²€ìƒ‰ ìˆ˜í–‰ í•¨ìˆ˜
    
    Args:
        user_info_uncertain: Trueì¼ ê²½ìš°, ì˜¤ë¡œì§€ ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸ë§Œ ê¸°ë°˜ìœ¼ë¡œ í‚¤ì›Œë“œ ìƒì„±.
                            ì´ì „ ê²€ìƒ‰ ê²°ê³¼ë‚˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ í‚¤ì›Œë“œ ìƒì„±ì— í¬í•¨í•˜ì§€ ì•ŠìŒ.
    """
    global model, processor

    user_query_for_prompt = (user_query or additional_context or initial_query or "").strip()
    current_query = initial_query
    best_result_content = "" # ê°€ì¥ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ê²€ìƒ‰ ê²°ê³¼ ì½˜í…ì¸ ë¥¼ ì €ì¥
    all_results_history = [] # ëª¨ë“  ë°˜ë³µì—ì„œì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë¡í•¨
    search_context_accumulated = [] # ì´ì „ ê²€ìƒ‰ ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸ ëˆ„ì 
    has_any_search_snippet = False

    # ëª¨ë¸/í”„ë¡œì„¸ì„œê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ì„ ë•ŒëŠ” ì™¸ë¶€ LLM í‰ê°€ ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ê³ 
    # ìˆœìˆ˜ êµ¬ê¸€ ìŠ¤ë‹ˆí« ê¸°ë°˜ ìš”ì•½ë§Œ ë°˜í™˜í•˜ë„ë¡ ì•ˆì „ í´ë°±
    model_ready = model is not None and processor is not None

    if user_info_uncertain:
        logging.info(f"[Recursive Search] user_info_uncertain=True - í‚¤ì›Œë“œëŠ” ì˜¤ë¡œì§€ ì‚¬ìš©ì ì§ˆë¬¸ë§Œ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤.")

    for i in range(max_iterations):
        logging.info(f"[Recursive Search] Iteration {i+1}/{max_iterations} - Current Query: '{current_query}'")
        
        # 1. ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±
        # user_info_uncertainì´ Trueì´ë©´ search_history_summaryë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        search_history_summary = None
        if not user_info_uncertain:
            search_history_summary = "\n".join(all_results_history[-2:]) # ìµœê·¼ 2ê°œ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
            if not search_history_summary:
                search_history_summary = None
        
        search_keywords_str = generate_search_keywords_langchain(
            user_query_for_prompt or initial_query, 
            current_query, 
            additional_context, 
            language,
            search_history_summary,
            strict_user_query_only=user_info_uncertain  # user_info_uncertainì¼ ë•Œ strict ëª¨ë“œ í™œì„±í™”
        )

        if search_keywords_str == "NO_BETTER_KEYWORDS" or not search_keywords_str.strip():
            logging.info(f"[Recursive Search] No better keywords generated. Ending search loop.")
            break # ë” ì´ìƒ ê°œì„ ëœ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ

        current_search_terms = [kw.strip() for kw in search_keywords_str.split(',') if kw.strip()]
        if not current_search_terms: # í‚¤ì›Œë“œê°€ íŒŒì‹±ë˜ì§€ ì•Šì•˜ì„ ë•Œ
            logging.warning(f"[Recursive Search] Keyword generation yielded no usable terms. Using last query for search.")
            current_search_terms = [current_query] # ìµœì†Œí•œ í˜„ì¬ ì¿¼ë¦¬ë¡œë¼ë„ ì‹œë„

        # 2. ìƒì„±ëœ í‚¤ì›Œë“œë¡œ êµ¬ê¸€ ê²€ìƒ‰ ìˆ˜í–‰
        snippets_this_iteration = []
        for term in current_search_terms:
            logging.debug(f"[Recursive Search] Searching Google for term: '{term}'")
            raw_search_results = google_search(term, num_results=3) # ê° í‚¤ì›Œë“œë‹¹ 3ê°œ ê²°ê³¼
            # ìŠ¤ë‹ˆí«ì— ì¶œì²˜(ë„ë©”ì¸)ì™€ ì œëª©ì„ í•¨ê»˜ í¬í•¨í•˜ì—¬ ê·¼ê±° ê°€ì‹œì„±ì„ ë†’ì„
            try:
                from urllib.parse import urlparse
            except Exception:
                urlparse = None
            for res in raw_search_results:
                snippet = res.get('snippet', '')
                if not snippet:
                    continue
                title = res.get('title', '')
                link = res.get('link', '')
                domain = ''
                if link and urlparse:
                    try:
                        domain = urlparse(link).netloc
                    except Exception:
                        domain = ''
                prefix = f"[{domain}] " if domain else ''
                title_part = f"{title} â€” " if title else ''
                enriched = f"{prefix}{title_part}{snippet}"
                snippets_this_iteration.append(enriched)

        combined_snippets_this_iteration = "\n\n".join([s for s in snippets_this_iteration if s.strip()])
        if not combined_snippets_this_iteration:
            logging.warning(f"[Recursive Search] No useful snippets found for terms: {current_search_terms}")
            all_results_history.append(f"No results for '{', '.join(current_search_terms)}'.")
            if i == max_iterations - 1: # ë§ˆì§€ë§‰ ì‹œë„ì¸ë° ê²°ê³¼ê°€ ì—†ìœ¼ë©´
                logging.info(f"[Recursive Search] Max iterations reached without satisfactory results.")
                if language == "ko":
                    best_result_content = "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ê´€ë ¨ ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."
                else:
                    best_result_content = "No search results found or insufficient relevant information."
            continue # ë‹¤ìŒ ë°˜ë³µìœ¼ë¡œ

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
        all_results_history.append(f"Results for '{', '.join(current_search_terms)}':\n{combined_snippets_this_iteration}")
        search_context_accumulated.append(combined_snippets_this_iteration)
        has_any_search_snippet = True

        # 3. LLMì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ í‰ê°€ ë° ë‹¤ìŒ ì¿¼ë¦¬ ì œì•ˆ (ì¬ê·€ì  ê°œì„ )
        # ì´ì „ ì»¨í…ìŠ¤íŠ¸ì™€ í˜„ì¬ ê²°ê³¼ë¥¼ ëª¨ë‘ ê³ ë ¤
        full_context_for_evaluation = (
            f"Original user query: {user_query_for_prompt or initial_query}\n"
            f"Latest search query: {current_query}\n"
            f"Additional context: {additional_context or 'None'}\n\n"
            "All search results so far:\n" + "\n\n".join(search_context_accumulated)
        )

        if language == "ko":
            eval_prompt = f"""
            ì›ë³¸ ì‚¬ìš©ì ìš”ì²­: "{user_query_for_prompt or initial_query}"
            ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸: {additional_context or "ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ"}
            í˜„ì¬ê¹Œì§€ì˜ ê²€ìƒ‰ ê²°ê³¼ ì¢…í•©:
            ---
            {full_context_for_evaluation}
            ---
            
            ì´ ê²°ê³¼ë“¤ì´ ì›ë³¸ ìš”ì²­ì— ë‹µí•˜ê¸°ì— ì¶©ë¶„í•œì§€ í‰ê°€í•˜ì„¸ìš”. ì¶©ë¶„í•˜ë©´ "ë§Œì¡± (SATISFACTORY)"ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.
            ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ë©´, ì›ë³¸ ì§ˆì˜ì˜ ì˜ë¯¸ë¥¼ ë³´ì¡´í•˜ë©´ì„œë„ ë” ì˜ ë‹µì„ ì°¾ì„ ìˆ˜ ìˆëŠ” **ìƒˆë¡œìš´ ê²€ìƒ‰ ì§ˆì˜** 2-3ê°œë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
            - ë„ë©”ì¸ ë¬´ê´€(ë‰´ìŠ¤/í•™ë¬¸/ê°€ì´ë“œ/ê²Œì„/ì„œì  ë“±)í•˜ê²Œ ì ìš© ê°€ëŠ¥í•œ êµ¬ì²´ì„± ìœ ì§€
            - ì¼ë°˜ì–´ë§Œ ë‹¨ë…ìœ¼ë¡œ ì“°ì§€ ë§ê³ , ì˜ë¯¸ìƒ í•µì‹¬ì–´ì™€ ê²°í•©
            - ì‰¼í‘œë¡œ êµ¬ë¶„
            ì‘ë‹µ í˜•ì‹: "ë¶€ì¡±í•œ ì : [ë‚´ìš©], ìƒˆ ì¿¼ë¦¬: [ì§ˆì˜1, ì§ˆì˜2]" ë˜ëŠ” "ë§Œì¡± (SATISFACTORY)"
            """
        else:
            eval_prompt = f"""
            Original user request: "{user_query_for_prompt or initial_query}"
            Additional context: {additional_context or "No additional context provided"}
            Accumulated search results so far:
            ---
            {full_context_for_evaluation}
            ---
            
            Decide sufficiency. If sufficient, return 'SATISFACTORY' only. If not, propose 2-3 new search queries that preserve the meaning and increase specificity, domain-agnostic. Separate with commas.
            Format: "Lacking: [description], New Query: [query1, query2]" or "SATISFACTORY".
            """

        # ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì€ ê²½ìš°: ìŠ¤ë‹ˆí« ê¸¸ì´/ì§ˆë¬¸ì–´ í¬í•¨ ì—¬ë¶€ë¡œ ê°„ë‹¨ í‰ê°€í•˜ì—¬ ê³„ì†/ì¢…ë£Œ ê²°ì •
        if not model_ready:
            long_enough = len(combined_snippets_this_iteration) > 400
            lowered_query = (user_query_for_prompt or initial_query or "").lower()
            has_question_words = any(w in lowered_query for w in ["what", "who", "why", "how", "when", "where"]) or any(w in (user_query_for_prompt or initial_query or "") for w in ["ì™œ", "ë¬´ì—‡", "ì–´ë””", "ì–¸ì œ", "ì–´ë–»ê²Œ"])
            if long_enough and has_question_words:
                eval_analysis = "SATISFACTORY"
            else:
                # ë‹¤ìŒ ë°˜ë³µì„ ìœ„í•œ ê°„ë‹¨ ìƒˆ í‚¤ì›Œë“œ ì œì•ˆ í˜•ì‹ ìœ ì§€
                # LLM ë¯¸ì¤€ë¹„ ì‹œì—ë„, ì›ë³¸ ì§ˆì˜ì˜ í•µì‹¬ ì–´íœ˜ë¥¼ ìœ ì§€í•œ ê°„ë‹¨í•œ ì¬ì§ˆì˜ë¥¼ êµ¬ì„±
                # ë„ˆë¬´ ì¼ë°˜ì ì¸ í† í°ë§Œ ë‚¨ì§€ ì•Šë„ë¡ íŠ¸ë¦¬ë°
                base = (user_query_for_prompt or initial_query or "")
                base = base if len(base) <= 80 else base[:80]
                fallback_terms = ", ".join([base] + current_search_terms[:1]) if current_search_terms else base
                eval_analysis = f"Lacking: heuristic, New Query: {fallback_terms}"
        else:
            eval_messages = [{"role": "user", "content": [{"type": "text", "text": eval_prompt}]}]
            eval_inputs = processor.apply_chat_template(eval_messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors="pt").to(model.device, dtype=torch.bfloat16)
            eval_input_len = eval_inputs["input_ids"].shape[-1]

            # ëª¨ë¸ ì¶”ë¡  ìˆ˜í–‰
            with torch.inference_mode():
                eval_generation = model.generate(**eval_inputs, max_new_tokens=200, do_sample=True, temperature=0.5)
                eval_generation = eval_generation[0][eval_input_len:]
            eval_analysis = processor.decode(eval_generation, skip_special_tokens=True).strip()
        logging.info(f"[Recursive Search] Evaluation for iteration {i+1}: '{eval_analysis}'")

        if "SATISFACTORY" in eval_analysis.upper() or "ë§Œì¡±" in eval_analysis:
            best_result_content = "\n\n".join(search_context_accumulated).strip()
            logging.info(f"[Recursive Search] Search deemed SATISFACTORY after {i+1} iterations.")
            return best_result_content, True, i + 1
        else:
            # ìƒˆë¡œìš´ ì¿¼ë¦¬ ì¶”ì¶œ
            new_query_match = re.search(r'(ìƒˆ ì¿¼ë¦¬|New Query):\s*([^,]+(?:,\s*[^,]+)*)', eval_analysis, re.IGNORECASE)
            if new_query_match:
                new_keywords_str = new_query_match.group(2).strip()
                current_query = new_keywords_str # ë‹¤ìŒ ë°˜ë³µì„ ìœ„í•œ ìƒˆ ì¿¼ë¦¬
                if not current_query.strip(): # ìƒˆ ì¿¼ë¦¬ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì¢…ë£Œ
                    logging.warning(f"[Recursive Search] Extracted new query is empty. Ending search.")
                    break
            else:
                logging.warning(f"[Recursive Search] Could not extract new query from evaluation. Ending search.")
                break # ìƒˆ ì¿¼ë¦¬ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìœ¼ë©´ ì¢…ë£Œ

    # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ë¥¼ ì±„ì› ê±°ë‚˜, ë” ì´ìƒ ê°œì„ ë  ì¿¼ë¦¬ê°€ ì—†ëŠ” ê²½ìš°
    final_content_for_response = "\n\n".join(search_context_accumulated).strip()
    
    if not has_any_search_snippet:
        logging.info(f"[Recursive Search] No web snippets gathered after {max_iterations} iterations.")
    else:
        logging.info(f"[Recursive Search] Max iterations reached or no further refinement. Returning best available content.")
    return final_content_for_response, False, max_iterations

# Google cse api ì‚¬ìš© (Google Custom Search Engine) https://programmablesearchengine.google.com
def google_search(query: str, num_results: int = 5) -> List[Dict[str, str]]:

    # ì„¤ì • íŒŒì¼ ë¡œë“œ (ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©ìœ¼ë¡œ FastAPI ëª¨ë“ˆ import ì‹œì—ë„ ì •ìƒ ì‘ë™)
    try:
        import os
        config = configparser.ConfigParser()
        # ì´ ìŠ¤í¬ë¦½íŠ¸ì˜ ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ config.ini ê²½ë¡œ ì„¤ì •
        config_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'config.ini')
        try:
            config.read(config_path, encoding='utf-8')
        except Exception:
            config.read(config_path)
        
        google_api_key = config['DEFAULT']['google_api_key'].strip()
        cx = config['DEFAULT']['cx'].strip()
        
        # API í‚¤ ìœ íš¨ì„± ê²€ì¦
        if not google_api_key:
            raise ValueError("google_api_key is empty or not set in config.ini")
        if not cx:
            raise ValueError("cx (Custom Search Engine ID) is empty or not set in config.ini")
            
        logging.debug(f"Config loaded from: {config_path}, API key length: {len(google_api_key)}, CX: {cx[:10]}...")
    except Exception as e:
        logging.error(f"Failed to load API configuration: {e}")
        # ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ˆ ê²°ê³¼ ë°˜í™˜
        return []

    # í•œê¸€ ì¿¼ë¦¬ëŠ” URL ì¸ì½”ë”©ì„ ëª…ì‹œì ìœ¼ë¡œ ì²˜ë¦¬
    import urllib.parse
    encoded_query = urllib.parse.quote(query)

    # í•œê¸€ ê²€ìƒ‰ì„ ìœ„í•œ ì¶”ê°€ ë§¤ê°œë³€ìˆ˜ ì„¤ì •
    has_korean = any('\uAC00' <= char <= '\uD7A3' for char in query)

   # í•œê¸€ ê²€ìƒ‰ ìµœì í™” ë§¤ê°œë³€ìˆ˜ (í•œê¸€ì´ ê°ì§€ëœ ê²½ìš°)
    if has_korean:
        url = f"https://www.googleapis.com/customsearch/v1?q={encoded_query}&key={google_api_key}&cx={cx}&hl=ko&lr=lang_ko&num={num_results}"
        logging.info(f"Korean query detected: {query}")
    else:
        url = f"https://www.googleapis.com/customsearch/v1?q={encoded_query}&key={google_api_key}&cx={cx}&num={num_results}"
    
    # url = f"https://www.googleapis.com/customsearch/v1?q={query}&key={google_api_key}&cx={cx}"
    
    # ë¡œê¹…ì—ì„œ API í‚¤ ë§ˆìŠ¤í‚¹
    log_url = url.replace(google_api_key, "API_KEY_MASKED")
    logging.info(f"Requesting Google API: {log_url}")

    try:
        response = requests.get(url)
        response.raise_for_status() # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚´
        search_results = response.json()
    except requests.RequestException as e:
        logging.error(f"Error occurred while requesting Google API: {e}")
        return []
    
    logging.debug(f"Google API response: {search_results}")

    # ê° ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê´€ë ¨ ì •ë³´ ì¶”ì¶œ
    results = []
    if 'items' in search_results:
        for item in search_results['items']:
            result = {
                'title': item.get('title', ''),
                'link': item.get('link', ''),
                'snippet': item.get('snippet', '')
            }

            try:
                # í…ìŠ¤íŠ¸ì— í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                has_korean = any('\uAC00' <= char <= '\uD7A3' for char in result['snippet'])
                if has_korean:
                    logging.debug(f"Korean text detected in snippet: {result['snippet'][:30]}...")
                else:
                    logging.debug(f"Non-Korean text in snippet: {result['snippet'][:30]}...")
            except Exception as e:
                logging.warning(f"Error checking text language: {e}")

            # Google APIì˜ ê²½ìš° ë””ì½”ë”© ë¶ˆí•„ìš”í•¨.

            # # Base64ë¡œ ì¸ì½”ë”©ëœ í•„ë“œê°€ ìˆë‹¤ë©´
            # if 'snippet' in item:
            #     encoded_text = item['snippet']
            #     try:
            #         # Base64 ë””ì½”ë”©
            #         decoded_text = base64.b64decode(encoded_text).decode('utf-8')
            #         result['snippet'] = decoded_text
            #     except Exception as e:
            #         # Base64 ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜
            #         print(f"Base64 decoding fail: {e}")

            results.append(result)
    else:
        logging.warning("No 'items' key in search results. Available keys: " + 
                        str(list(search_results.keys())))
        # ì—ëŸ¬ ëŒ€ì‹  ê¸°ë³¸ ì‘ë‹µì„ ìƒì„±
        results = [{
            'title': 'ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ',
            'link': '',
            'snippet': 'ì´ ê²€ìƒ‰ì–´ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¼ì¼ í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆê±°ë‚˜ ê²€ìƒ‰ì–´ê°€ ë„ˆë¬´ ëª¨í˜¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'
        }]

    logging.debug(f"Search results: {results}")
    return results


import logging as log

# === ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ í›… (ìˆìœ¼ë©´ ì‚¬ìš©) ===
def _call_langgraph_pipeline(query: str, problem_type: str, language: str) -> Optional[dict]:
    """
    ê¸°ì¡´ ì½”ë“œë² ì´ìŠ¤ì— ì •ì˜ëœ `search_and_reason_for_complex_problem_langgraph`ê°€ ìˆìœ¼ë©´ ì‚¬ìš©.
    ì—†ê±°ë‚˜ ì‹¤íŒ¨í•˜ë©´ None.
    """
    try:
        fn = globals().get('search_and_reason_for_complex_problem_langgraph')
        if callable(fn):
            return fn(
                query=query,
                problem_type=problem_type,
                additional_context=None,
                max_iterations=1,
                language=language
            )
    except Exception as e:
        import traceback
        log.warning("LangGraph pipeline failed: %s\n%s", e, traceback.format_exc())
    return None




# ì£¼ì–´ì§„ URLì—ì„œ ì£¼ìš” í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ì¶”ì¶œ
def extract_content_from_url(url: str) -> str:
    try:
        response = requests.get(url, timeout=5)

        # HTML ë‚´ìš© íŒŒì‹±
        soup = BeautifulSoup(response.content, 'html.parser')

        # scriptì™€ style ìš”ì†Œë¥¼ ì œê±°
        for script in soup(["script", "style"]):
            script.decompose()

        # íŒŒì‹±ëœ HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = soup.get_text()

        # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì •ë¦¬
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = '\n'.join(chunk for chunk in chunks if chunk)
        
        logging.debug(f"Length of text extracted from URL {url}: {len(text)}")
        return text
    except Exception as e:
        print(f"Error extracting content from {url}: {e}")
        return ""

def get_relevant_information(query: str) -> List[Dict[str, str]]:

    global search_results

    # Google ê²€ìƒ‰ ìˆ˜í–‰
    search_results = google_search(query)
    relevant_info = [] # ê´€ë ¨ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸

    # ê° ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë‚´ìš©ì„ ì¶”ì¶œ
    for result in search_results:
        content = extract_content_from_url(result['link'])
        relevant_info.append({
            'title': result['title'],
            'content': content[:1000],
            'url': result['link'],
            'snippet': result.get('snippet', '') # ë””ì½”ë”©ëœ ìŠ¤ë‹ˆí« í¬í•¨
        })

    logging.debug(f"Number of relevant information extracted for query '{query}': {len(relevant_info)}")
    return relevant_info

def process_with_rag(query: str, additional_context: Optional[str] = None, max_context_length: int = 1000, language=None) -> str:
    """RAG ì‹œìŠ¤í…œì„ ì‚¬ìš©, ê´€ë ¨ ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ê³  í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±"""

    # ì–¸ì–´ ìë™ ê°ì§€ (ì–¸ì–´ê°€ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš°)
    if language is None:
        # Raika_Gemma_FastAPIì˜ detect_language í•¨ìˆ˜ ì‚¬ìš©
        from Raika_Gemma_FastAPI import detect_language
        language = detect_language(query)

    try:
        # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        rag = RAGSystem(max_context_length=max_context_length, language=language)

        # ì¬ê·€ì  ê²€ìƒ‰ ìˆ˜í–‰
        relevant_info, is_satisfactory, iterations = recursive_search(
            query,
            additional_context or "",
            language=language,
            user_query=query
        )

        # ë¹ˆ ê²°ê³¼ ì²´í¬
        if not relevant_info.strip():
            if language == "ko":
                return "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‚´ë¶€ ì§€ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µí•˜ê² ìŠµë‹ˆë‹¤."
            else:
                return "No search results found. I'll respond based on my internal knowledge."

        # RAG ì‹œìŠ¤í…œì— ë¬¸ì„œë¥¼ ì¶”ê°€
        rag.add_documents([relevant_info])

        # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì²­í¬ ê°€ì ¸ì˜¤ê¸°
        relevant_chunks = rag.get_relevant_chunks(query)

        # ê´€ë ¨ ì²­í¬ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        if not relevant_chunks or all(not chunk.strip() for chunk in relevant_chunks):
            if language == "ko":
                return "ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‚´ë¶€ ì§€ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µí•˜ê² ìŠµë‹ˆë‹¤."
            else:
                return "Could not extract relevant information from search results. I'll respond based on my internal knowledge."

        # ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±
        if is_satisfactory:
            prompt = rag.create_prompt(query, relevant_chunks, language=language)
            
            # ë¼ì´ì¹´ ìºë¦­í„° ìœ ì§€ë¥¼ ìœ„í•œ ì•ˆë‚´ ì¶”ê°€ (í•œì˜ ëª¨ë‘)
            if language == "ko":
                prompt += "\n(ìš°ì„ ìˆœìœ„: ëŠ‘ëŒ€ê°œ ë¼ì´ì¹´ ìºë¦­í„°ë¥¼ ìœ ì§€í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤. ì‚¬ì‹¤ ì •ë³´ë¥¼ ì œê³µí•˜ê±°ë‚˜ RAG ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ë•Œì—ë„ í•­ìƒ ìºë¦­í„°ë¥¼ ìœ ì§€í•˜ì„¸ìš”.)"
            else:
                prompt += "\n(Priority: Maintaining Raika's wolfdog character is the highest priority. Even when providing factual information or using RAG systems, always stay in character.)"
        else:
            if language == "ko":
                # í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸
                additional_context_prompt = f" ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸: '{additional_context}'" if additional_context else ""
                prompt = f"""
                {iterations}ë²ˆì˜ ê²€ìƒ‰ ê²°ê³¼ ê°œì„  ì‹œë„ í›„ì—ë„, ì¿¼ë¦¬ì— ëŒ€í•œ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: "{query}"{additional_context_prompt}. 
                í•˜ì§€ë§Œ ì°¾ì€ ê°€ì¥ ì¢‹ì€ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

                {rag.create_prompt(query, relevant_chunks, language=language)}

                ì´ ê²°ê³¼{' ë° ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸' if additional_context else ''}ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ëŠ¥í•œ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”. ê·¸ë¦¬ê³  ì •ë³´ê°€ ì™„ì „í•˜ê±°ë‚˜ ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒì„ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.
                """
                prompt += "\n(ìš°ì„ ìˆœìœ„: ëŠ‘ëŒ€ê°œ ë¼ì´ì¹´ ìºë¦­í„°ë¥¼ ìœ ì§€í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤. ì‚¬ì‹¤ ì •ë³´ë¥¼ ì œê³µí•˜ê±°ë‚˜ RAG ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ë•Œì—ë„ í•­ìƒ ìºë¦­í„°ë¥¼ ìœ ì§€í•˜ì„¸ìš”.)"
            else:
                # ì˜ì–´ í”„ë¡¬í”„íŠ¸ (ê¸°ì¡´)
                additional_context_prompt = f" with additional context: '{additional_context}'" if additional_context else ""
                prompt = f"""
                After {iterations} attempts to improve the search results, we couldn't find fully satisfactory information for the query: "{query}"{additional_context_prompt}. 
                However, here are the best results we found:

                {rag.create_prompt(query, relevant_chunks, language=language)}

                Please provide the most relevant information possible based on these results{' and the additional context' if additional_context else ''}, and acknowledge that the information might not be complete or fully accurate.
                """ 
                prompt += "\n(Priority: Maintaining Raika's wolfdog character is the highest priority. Even when providing factual information or using RAG systems, always stay in character.)"

        return prompt
    
    except Exception as e:
        logging.error(f"Error in RAG processing: {str(e)}")
        if language == "ko":
            return f"ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}. ë‚´ë¶€ ì§€ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µí•˜ê² ìŠµë‹ˆë‹¤."
        else:
            return f"An error occurred during search processing: {str(e)}. I'll respond based on my internal knowledge."

# í”„ë¡œì„¸ì„œì™€ ëª¨ë¸ ë¡œë“œë¥¼ ìœ„í•œ í•¨ìˆ˜ (ë…ë¦½ ì‹¤í–‰ ì‹œ ì‚¬ìš©)
def load_model_for_testing():
    global model, processor
    
    # model_id = "google/gemma-3-4b-it"
    model_id = "unsloth/gemma-3-12b-it-bnb-4bit"
    # ì§€ì—° ì„í¬íŠ¸: torchvision ë¹„ì˜ì¡´ ê²½ë¡œë¡œë§Œ ì‚¬ìš©
    from transformers import AutoProcessor, Gemma3ForConditionalGeneration
    processor = AutoProcessor.from_pretrained(model_id)
    model = Gemma3ForConditionalGeneration.from_pretrained(
        model_id,
        device_map="auto",
        torch_dtype=torch.bfloat16
    ).eval()
    
    print("Model and processor loaded successfully for testing.")
    return model, processor


# TODO: (25.05.18) LangGraphì˜ ì˜¨ì „í•œ êµ¬í˜„

from typing import TypedDict

# """ --- LangGraphë¥¼ ìœ„í•œ ìƒíƒœ ì •ì˜ (25.05.18) --- """
class ComplexSearchGraphState(TypedDict):
    # í•„ìˆ˜ ì…ë ¥ ê°’
    original_query: str
    problem_type: str # e.g., "complex_math_problem"
    language: str
    max_iterations: int # LangGraph ë£¨í”„ì˜ ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜

    # ì„ íƒì  ì…ë ¥ ê°’
    additional_context: Optional[str]
    user_info_uncertain: Optional[bool] # Trueì´ë©´ ì˜¤ë¡œì§€ ì‚¬ìš©ì ì§ˆë¬¸ë§Œ ê¸°ë°˜ìœ¼ë¡œ í‚¤ì›Œë“œ ìƒì„±

    # ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ì—…ë°ì´íŠ¸ë˜ëŠ” ê°’
    current_search_query: str # í˜„ì¬ ê²€ìƒ‰ì— ì‚¬ìš©í•  ì¿¼ë¦¬
    iteration_count: int # í˜„ì¬ ë°˜ë³µ íšŸìˆ˜
    reasoning_log: List[str] # ì¶”ë¡  ê³¼ì •ì„ ê¸°ë¡í•˜ëŠ” ë¡œê·¸
    search_results_snippets: List[str] # í˜„ì¬ ë°˜ë³µì—ì„œì˜ ê²€ìƒ‰ ê²°ê³¼ ìŠ¤ë‹ˆí«ë“¤
    relevant_snippets_evaluation: List[dict] # [{'text': snippet, 'score': score, 'reason': reason}]
    current_best_snippet_from_iteration: Optional[dict] # í˜„ì¬ ë°˜ë³µì—ì„œ ê°€ì¥ ì¢‹ì€ ìŠ¤ë‹ˆí« ì •ë³´
    current_plan: Optional[str] # í˜„ì¬ ìŠ¤ë‹ˆí« ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ëœ ê³„íš
    current_plan_evaluation: Optional[dict] # {'is_sound': bool, 'critique': str, 'confidence': int}

    # ì „ì²´ ì‹¤í–‰ ì¤‘ ê°€ì¥ ì¢‹ì•˜ë˜ ê²°ê³¼
    best_overall_snippet_text: Optional[str]
    best_overall_plan_text: Optional[str]
    highest_overall_confidence: int

    # ìµœì¢… ì¶œë ¥
    final_output_for_raika: Optional[Dict[str, any]] # Raika ì—ì´ì „íŠ¸ì—ê²Œ ì „ë‹¬ë  ìµœì¢… ê²°ê³¼


# """ --- LangGraph ë…¸ë“œ í•¨ìˆ˜ ì •ì˜ (25.05.18) --- """

def node_initialize_graph(state: ComplexSearchGraphState) -> ComplexSearchGraphState:
    """ê·¸ë˜í”„ ì‹¤í–‰ ì‹œì‘ ì‹œ ìƒíƒœë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ë…¸ë“œ"""
    logging.info(f"[Graph] Initializing state for query: '{state['original_query']}'")
    return {
        **state, # ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì˜¨ ê°’ ìœ ì§€
        "current_search_query": state["original_query"], # ì²« ê²€ìƒ‰ì€ ì›ë³¸ ì¿¼ë¦¬ë¡œ ì‹œì‘
        "iteration_count": 0,
        "reasoning_log": [f"LangGraph ë³µí•© ê²€ìƒ‰ ì‹œì‘: '{state['original_query']}' (ìœ í˜•: {state['problem_type']}, ì–¸ì–´: {state['language']})"],
        "search_results_snippets": [],
        "relevant_snippets_evaluation": [],
        "current_best_snippet_from_iteration": None,
        "current_plan": None,
        "current_plan_evaluation": None,
        "best_overall_snippet_text": None,
        "best_overall_plan_text": None,
        "highest_overall_confidence": -1, # -1ë¡œ ì´ˆê¸°í™”í•˜ì—¬ ì–´ë–¤ ìœ íš¨í•œ ì ìˆ˜ë“  ë” ë†’ê²Œ ì²˜ë¦¬
        "final_output_for_raika": None
    }

def node_perform_search(state: ComplexSearchGraphState) -> ComplexSearchGraphState:
    """í˜„ì¬ ì¿¼ë¦¬ë¡œ êµ¬ê¸€ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  ìŠ¤ë‹ˆí«ì„ ì¶”ì¶œí•˜ëŠ” ë…¸ë“œ"""
    log = state["reasoning_log"]
    current_query = state["current_search_query"]
    lang = state["language"]
    iter_count = state["iteration_count"]

    log.append(f"\n[ë°˜ë³µ {iter_count+1}] ê²€ìƒ‰ ìˆ˜í–‰: '{current_query}'")
    logging.info(f"[Graph][Iter {iter_count+1}] Performing search for: '{current_query}'")

    # recursive_search ë˜ëŠ” Google Search ì§ì ‘ ì‚¬ìš© ê°€ëŠ¥.
    # ì—¬ê¸°ì„œëŠ” Google Searchë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ìŠ¤ë‹ˆí« ë¦¬ìŠ¤íŠ¸ë¥¼ ì–»ê³ , ë‹¤ìŒ ë…¸ë“œì—ì„œ RAG/í‰ê°€ ìˆ˜í–‰
    search_results_items = google_search(current_query, num_results=5) # 5ê°œ ê²°ê³¼ ìš”ì²­
    snippets = [item.get('snippet', '') for item in search_results_items if item.get('snippet')]

    if not snippets:
        log.append("- ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìŠ¤ë‹ˆí«ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        logging.warning(f"[Graph][Iter {iter_count+1}] No snippets found for query '{current_query}'.")
    else:
        log.append(f"- {len(snippets)}ê°œì˜ ìŠ¤ë‹ˆí« ì°¾ìŒ.")
        logging.debug(f"[Graph][Iter {iter_count+1}] Found snippets: {[s[:50] + '...' for s in snippets]}")

    return {**state, "search_results_snippets": snippets}

def node_evaluate_snippets(state: ComplexSearchGraphState) -> ComplexSearchGraphState:
    """ê²€ìƒ‰ëœ ìŠ¤ë‹ˆí«ë“¤ì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ëŠ” ë…¸ë“œ"""
    log = state["reasoning_log"]
    snippets = state["search_results_snippets"]
    original_query = state["original_query"]
    lang = state["language"]
    iter_count = state["iteration_count"]

    log.append(f"[ë°˜ë³µ {iter_count+1}] ìŠ¤ë‹ˆí« ê´€ë ¨ì„± í‰ê°€ ì¤‘...")
    logging.info(f"[Graph][Iter {iter_count+1}] Evaluation {len(snippets)} snippets...")

    evaluated_this_iteration = []
    if not snippets:
        log.append("- í‰ê°€í•  ìŠ¤ë‹ˆí«ì´ ì—†ìŠµë‹ˆë‹¤.")
        return {**state, "relevant_snippets_evaluation": [], "current_best_snippet_from_iteration": None}
    
    for snippet in snippets[:3]: # ì‹œê°„ íš¨ìœ¨ìƒ ìµœëŒ€ 3ê°œ ìŠ¤ë‹ˆí« í‰ê°€
        if not snippet.strip():
            continue
        is_relevant, reason, score = evaluate_relevance(original_query, snippet, lang)
        log.append(f"   - ìŠ¤ë‹ˆí« í‰ê°€: ê´€ë ¨ì„±={is_relevant}, ì ìˆ˜={score}, ì´ìœ ='{reason}', ë‚´ìš©='{snippet[:60].replace(chr(10), ' ')}...'")
        if is_relevant and score >= 5: # ê´€ë ¨ì„± ì„ê³„ê°’
            evaluated_this_iteration.append({'text': snippet, 'score': score, 'reason': reason})

    # í˜„ì¬ ë°˜ë³µì—ì„œ ê°€ì¥ ì ìˆ˜ê°€ ë†’ì€ ìŠ¤ë‹ˆí« ì„ íƒ
    current_best_snippet = None
    if evaluated_this_iteration:
        evaluated_this_iteration.sort(key=lambda x: x['score'], reverse=True)
        current_best_snippet = evaluated_this_iteration[0] # ê°€ì¥ ì ìˆ˜ê°€ ë†’ì€ ê²ƒ
        log.append(f"- ì´ë²ˆ ë°˜ë³µì˜ ìµœì  ìŠ¤ë‹ˆí« (ì ìˆ˜: {current_best_snippet['score']}): '{current_best_snippet['text'][:60].replace(chr(10), ' ')}...'")
        logging.info(f"[Graph][Iter {iter_count+1}] Best snippet from this iteration (Score: {current_best_snippet['score']}).")
    else:
        log.append("- ì´ë²ˆ ë°˜ë³µì—ì„œ ìœ ì˜ë¯¸í•œ ê´€ë ¨ ìŠ¤ë‹ˆí«ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        logging.warning(f"[Graph][Iter {iter_count+1}] No significantly relevant snippets found in this iteration.")

    return {**state, "relevant_snippets_evaluation": evaluated_this_iteration, "current_best_snippet_from_iteration": current_best_snippet}

def node_generate_and_evaluate_plan(state: ComplexSearchGraphState) -> ComplexSearchGraphState:
    """ìµœì  ìŠ¤ë‹ˆí«ì„ ê¸°ë°˜ìœ¼ë¡œ ê³„íšì„ ìƒì„±í•˜ê³  í‰ê°€í•˜ëŠ” ë…¸ë“œ"""
    log = state["reasoning_log"]
    best_snippet_info = state["current_best_snippet_from_iteration"]
    original_query = state["original_query"]
    lang = state["language"]
    iter_count = state["iteration_count"]

    current_plan_text = None
    current_plan_eval = None

    if not best_snippet_info or not best_snippet_info.get('text'):
        log.append(f"[ë°˜ë³µ {iter_count+1}] ê³„íš ìƒì„±ì„ ìœ„í•œ ìŠ¤ë‹ˆí«ì´ ì—†ìŠµë‹ˆë‹¤.")
        logging.warning(f"[Graph][Iter {iter_count+1}] No snippet available for plan generation.")
        current_plan_eval = {'is_sound': False, 'critique': "ì •ë³´ ë¶€ì¡±ìœ¼ë¡œ ê³„íš ìƒì„± ë¶ˆê°€", 'confidence': 0}
    else:
        snippet_text = best_snippet_info['text']
        log.append(f"[ë°˜ë³µ {iter_count+1}] ìŠ¤ë‹ˆí« ê¸°ë°˜ ê³„íš ìƒì„± ì¤‘: '{snippet_text[:60].replace(chr(10), ' ')}...'")
        logging.info(f"[Graph][Iter {iter_count+1}] Generating plan based on snippet (Score: {best_snippet_info['score']}).")

        plan_text = plan_application(original_query, snippet_text, lang)
        if "Error generating plan" in plan_text:
            log.append(f"- ê³„íš ìƒì„± ì¤‘ ì˜¤ë¥˜: {plan_text}")
            current_plan_text = None
            current_plan_eval = {'is_sound': False, 'critique': f"ê³„íš ìƒì„± ì˜¤ë¥˜: {plan_text}", 'confidence': 0}
        else:
            current_plan_text = plan_text
            log.append(f"- ìƒì„±ëœ ê³„íš:\n{plan_text}")
            logging.debug(f"[Graph][Iter {iter_count+1}] Generated plan:\n{plan_text}")

            # ê³„íš í‰ê°€
            log.append(" - ê³„íš í‰ê°€ ì¤‘...")
            is_sound, critique, confidence = evaluate_plan(original_query, plan_text, lang)
            current_plan_eval = {'is_sound': is_sound, 'critique': critique, 'confidence': confidence}
            log.append(f" - ê³„íš í‰ê°€ ê²°ê³¼: í•©ë¦¬ì„±={is_sound}, ì‹ ë¢°ë„={confidence}, ê°œì„ ì ='{critique}'")
            logging.info(f"[Graph][Iter {iter_count+1}] Plan evaluation: Sound={is_sound}, Confidence={confidence}, Critique='{critique}'.")
    
    return {**state, "current_plan": current_plan_text, "current_plan_evaluation": current_plan_eval}

def node_update_overall_best(state: ComplexSearchGraphState) -> ComplexSearchGraphState:
    """í˜„ì¬ ë°˜ë³µì˜ ê³„íš í‰ê°€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ì²´ ìµœì í•´ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ë…¸ë“œ"""
    log = state["reasoning_log"]
    current_eval = state["current_plan_evaluation"]
    current_plan = state["current_plan"]
    current_snippet_info = state["current_best_snippet_from_iteration"]
    iter_count = state["iteration_count"]

    highest_overall_confidence = state.get("highest_overall_confidence", -1)
    best_overall_plan = state.get("best_overall_plan_text")
    best_overall_snippet = state.get("best_overall_snippet_text")

    if current_eval and current_eval.get('is_sound') and current_eval.get('confidence', 0) > highest_overall_confidence:
        new_highest_confidence = current_eval['confidence']
        log.append(f"[ë°˜ë³µ {iter_count+1}] ìƒˆë¡œìš´ ìµœì ì˜ ê³„íšì„ ë°œê²¬í•¨! ì‹ ë¢°ë„: {new_highest_confidence}, ì´ì „ ìµœê³  ì‹ ë¢°ë„: {highest_overall_confidence})")
        logging.info(f"[Graph][Iter {iter_count+1}] New best solution found with confidence {new_highest_confidence}.")
        return {
            **state,
            "highest_overall_confidence": new_highest_confidence,
            "best_overall_plan_text": current_plan,
            "best_overall_snippet_text": current_snippet_info.get('text') if current_snippet_info else None
        }
    else:
        if current_eval:
            log.append(f"[ë°˜ë³µ {iter_count+1}] ì´ë²ˆ ë°˜ë³µì˜ ê³„íšì€ ê¸°ì¡´ ìµœì  ê³„íšì„ ë„˜ì–´ì„œì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (í˜„ì¬ ì‹ ë¢°ë„: {current_eval.get('confidence', 0)}, ì „ì²´ ìµœê³  ì‹ ë¢°ë„: {highest_overall_confidence}).")
        else:
            log.append(f"[ë°˜ë³µ {iter_count+1}] ê³„íš í‰ê°€ ì •ë³´ê°€ ì—†ì–´ ìµœì í•´ë¥¼ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return state # ë³€ê²½ ì—†ìŒ
    
def node_refine_search_query(state: ComplexSearchGraphState) -> ComplexSearchGraphState:
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ ë‹¤ìŒ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ëŠ” ë…¸ë“œ"""
    log = state["reasoning_log"]
    original_query = state["original_query"]
    # ì´ì „ ê²€ìƒ‰ ì¿¼ë¦¬ëŠ” current_search_queryì— ì´ë¯¸ ìˆìŒ.
    additional_context = state.get("additional_context")
    lang = state["language"]
    iter_count = state["iteration_count"]
    user_info_uncertain = state.get("user_info_uncertain", False)

    # user_info_uncertainì´ Trueì´ë©´ search_historyë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    search_history_summary = None
    if not user_info_uncertain:
        # ì´ì „ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½: ëª¨ë“  í‰ê°€ëœ ìŠ¤ë‹ˆí«ì˜ ì´ìœ ì™€ ì ìˆ˜, ê·¸ë¦¬ê³  ê°€ì¥ ìµœê·¼ ê³„íšì˜ ë¹„í‰ì„ ì‚¬ìš©
        critique_info = ""
        if state.get("current_plan_evaluation"):
            critique_info = f"ìµœê·¼ ê³„íšì˜ ë¬¸ì œì : {state['current_plan_evaluation'].get('critique', 'ì—†ìŒ')}."

        relevance_summary_parts = []
        for eval_snip in state.get("relevant_snippets_evaluation", []):
            relevance_summary_parts.append(f"ì •ë³´ ì¡°ê°(ì ìˆ˜ {eval_snip['score']}): '{eval_snip['text'][:50].replace(chr(10), ' ')}...' (ì´ìœ : {eval_snip['reason']})")
        relevance_summary = "\n".join(relevance_summary_parts)
        if not relevance_summary: relevance_summary = "ê´€ë ¨ ì •ë³´ ì¡°ê°ì„ ì°¾ì§€ ëª»í–ˆê±°ë‚˜ í‰ê°€ ì •ë³´ê°€ ì—†ìŒ"

        search_history_summary = f"{critique_info}ì´ì „ ê²€ìƒ‰ì—ì„œ í‰ê°€ëœ ì •ë³´: {relevance_summary}"
        search_history_summary = search_history_summary[:1000]

    log.append(f"[ë°˜ë³µ {iter_count+1}] ë‹¤ìŒ ê²€ìƒ‰ì„ ìœ„í•œ ìƒˆ í‚¤ì›Œë“œ ìƒì„± ì‹œë„...")
    logging.info(f"[Graph][Iter {iter_count+1}] Attempting to generate new keywords.")

    new_keywords = generate_search_keywords_langchain(
        original_query,
        state["current_search_query"], # ì´ì „ ê²€ìƒ‰ì–´
        additional_context,
        lang,
        search_history_summary=search_history_summary,
        strict_user_query_only=user_info_uncertain
    )

    if new_keywords == "NO_BETTER_KEYWORDS" or not new_keywords.strip() or new_keywords == state["current_search_query"]:
        log.append("- ë” ì´ìƒ ê°œì„ ëœ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì´ì „ ê²€ìƒ‰ì–´ë¥¼ ìœ ì§€í•˜ê±°ë‚˜ ê²€ìƒ‰ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        logging.warning(f"[Graph][Iter {iter_count+1}] No new keywords generated or same as previous. Signal to potentially end.")
        # ì´ ê²½ìš°, conditional_edge_decide_next_stepì—ì„œ ë£¨í”„ ì¢…ë£Œë¥¼ ê²°ì •í•  ìˆ˜ ìˆë„ë¡ current_search_queryë¥¼ ë³€ê²½í•˜ì§€ ì•Šê±°ë‚˜,
        # íŠ¹ë³„í•œ í”Œë˜ê·¸ë¥¼ ìƒíƒœì— ì¶”ê°€í•  ìˆ˜ ìˆìŒ. ì—¬ê¸°ì„œëŠ” current_search_queryë¥¼ ê·¸ëŒ€ë¡œ ë‘ .
        # ë˜ëŠ”, "NO_MORE_REFINEMENT" ê°™ì€ ìƒíƒœë¥¼ ì¶”ê°€í•˜ì—¬ ëª…ì‹œì ìœ¼ë¡œ ì•Œë¦¼
        return {**state, "current_search_query": "FINAL_ATTEMPT_NO_REFINEMENT_POSSIBLE"} # íŠ¹ìˆ˜ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë£¨í”„ ì¢…ë£Œ ìœ ë„
    else:
        log.append(f"- ìƒì„±ëœ ìƒˆ ê²€ìƒ‰ì–´: '{new_keywords}'")
        logging.info(f"[Graph][Iter {iter_count+1}] Generated new search query: '{new_keywords}'")
        return {**state, "current_search_query": new_keywords, "iteration_count": state["iteration_count"] + 1}

def node_prepare_final_output(state: ComplexSearchGraphState) -> ComplexSearchGraphState:
    """ìµœì¢… ê²°ê³¼ë¥¼ Raika ì—ì´ì „íŠ¸ í˜•ì‹ì— ë§ê²Œ ì¤€ë¹„í•˜ëŠ” ë…¸ë“œ"""
    log = state["reasoning_log"]
    log.append("\nìµœì¢… ê²°ê³¼ ì¤€ë¹„ ì¤‘...")
    logging.info("[Graph] Preparing final output for Raika.")

    final_reasoning_summary_str = "\n".join(log)

    if state.get("best_overall_plan_text") and state.get("highest_overall_confidence", -1) >= 5: # ì„±ê³µ ì„ê³„ê°’
        final_output = {
            "status": "success",
            "query": state["original_query"],
            "best_snippet": state.get("best_overall_snippet_text", "í•´ë‹¹ ì—†ìŒ"),
            "best_plan": state["best_overall_plan_text"],
            "reasoning_summary": final_reasoning_summary_str,
            "confidence": state["highest_overall_confidence"],
            "language": state["language"]
        }
        log.append(f"ì„±ê³µì ì¸ ê²°ê³¼ ìƒì„± (ì‹ ë¢°ë„: {state['highest_overall_confidence']}).")
    else:
        final_output = {
            "status": "failure",
            "query": state["original_query"],
            "reasoning_summary": final_reasoning_summary_str,
            "message": "ì¶©ë¶„íˆ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê³„íšì„ ìˆ˜ë¦½í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤." if state["language"] == "ko" \
                        else "Could not formulate a confident plan based on search results.",
            "language": state["language"],
            "highest_confidence_achieved": state.get("highest_overall_confidence", -1)
        }
        log.append(f"ë§Œì¡±ìŠ¤ëŸ¬ìš´ ê³„íš ìˆ˜ë¦½ ì‹¤íŒ¨ (ìµœê³  ì‹ ë¢°ë„: {state.get('highest_overall_confidence', -1)}).")

    return {**state, "final_output_for_raika": final_output}


# --- LangGraph ì¡°ê±´ë¶€ ì—£ì§€ í•¨ìˆ˜ ---
def conditional_edge_decide_next_step(state: ComplexSearchGraphState) -> str:
    """ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ëŠ” ì¡°ê±´ë¶€ ì—£ì§€ ë¡œì§"""
    iter_count = state["iteration_count"] # initializeì—ì„œ 0ìœ¼ë¡œ ì‹œì‘í•˜ê³  refine queryì—ì„œ 1 ì¦ê°€
    max_iters = state["max_iterations"]
    current_plan_eval = state.get("current_plan_evaluation")
    current_query = state.get("current_search_query", "")

    logging.debug(f"[Graph][Router] Iter: {iter_count}, Max: {max_iters}, Confidence: {current_plan_eval.get('confidence') if current_plan_eval else 'N/A'}, Query: '{current_query}'")


    # current_search_queryê°€ íŠ¹ìˆ˜ ê°’ì´ë©´ ë¬´ì¡°ê±´ ì¢…ë£Œ
    if current_query == "FINAL_ATTEMPT_NO_REFINEMENT_POSSIBLE":
        logging.info("[Graph][Router] No more query refinement possible. Ending.")
        return "prepare_output_node" # ìµœì¢… ì¶œë ¥ ë…¸ë“œë¡œ
    
    if iter_count >= max_iters:
        logging.info(f"[Graph][Router] Max iterations ({max_iters}) reached. Proceeding to output.")
        return "prepare_output_node" # ìµœì¢… ì¶œë ¥ ë…¸ë“œë¡œ
    
    if current_plan_eval:
        confidence = current_plan_eval.get('confidence', 0)
        is_sound = current_plan_eval.get('is_sound', False)
        # ë†’ì€ ì‹ ë¢°ë„ì˜ ê³„íšì´ ì´ë¯¸ ë°œê²¬ëœ ê²½ìš° (ì „ì²´ ìµœê³  ì‹ ë¢°ë„ ë˜ëŠ” í˜„ì¬ ì‹ ë¢°ë„ ê¸°ì¤€)
        current_confidence = current_plan_eval.get('confidence', 0) if current_plan_eval else 0
        if current_confidence >= 8 or state.get("highest_overall_confidence", -1) >= 8:
            logging.info(f"[Graph][Router] High confidence solution found (Current: {current_confidence}, Overall: {state.get('highest_overall_confidence', -1)}). Ending.")
            return "prepare_output_node"

        # í˜„ì¬ ë°˜ë³µì˜ ê³„íšì´ ì¢‹ì§€ ì•Šê±°ë‚˜, ìŠ¤ë‹ˆí«ì´ ì—†ì—ˆê±°ë‚˜, ê³„íš ìƒì„±ì´ ì•ˆëœ ê²½ìš° -> ì¿¼ë¦¬ ê°œì„  ì‹œë„
        if not is_sound or confidence < 5 or not state.get("current_plan"):
            logging.info(f"[Graph][Router] Current plan is not good enough (Sound: {is_sound}, Conf: {confidence}) or no plan/snippet. Refining query.")
            return "refine_query_node" # ê²€ìƒ‰ì–´ ê°œì„  ë…¸ë“œë¡œ

    # ìœ„ ì¡°ê±´ì— í•´ë‹¹í•˜ì§€ ì•Šìœ¼ë©´ ë‹¤ìŒ ê²€ìƒ‰ ë°˜ë³µ (ì‹¤ì œë¡œëŠ” refine_query_nodeë¥¼ ê±°ì³ perform_search_nodeë¡œ ê°)
    # ëª…ì‹œì ìœ¼ë¡œ ê²€ìƒ‰ì–´ ê°œì„ ì„ ë¨¼ì € ì‹œë„í•˜ë„ë¡ refine_query_nodeë¡œ ë³´ëƒ„
    logging.info(f"[Graph][Router] Defaulting to query refinement or next iteration.")
    return "refine_query_node"

# LangGraph ë¹Œë” í•¨ìˆ˜
def build_complex_search_graph():
    """ë³µí•© ê²€ìƒ‰ ë° ì¶”ë¡ ì„ ìœ„í•œ LangGraphë¥¼ ë¹Œë“œí•˜ê³  ì»´íŒŒì¼í•¨."""
    if not model or not processor:
        raise ValueError("Model and Processor must be set before building the graph.")
    
    graph_builder = StateGraph(ComplexSearchGraphState)

    # ë…¸ë“œ ì¶”ê°€
    graph_builder.add_node("initialize_node", node_initialize_graph)
    graph_builder.add_node("perform_search_node", node_perform_search)
    graph_builder.add_node("evaluate_snippets_node", node_evaluate_snippets)
    graph_builder.add_node("generate_evaluate_plan_node", node_generate_and_evaluate_plan)
    graph_builder.add_node("update_overall_best_node", node_update_overall_best)
    graph_builder.add_node("refine_query_node", node_refine_search_query)
    graph_builder.add_node("prepare_output_node", node_prepare_final_output)

    # ì§„ì…ì  ì„¤ì •
    graph_builder.set_entry_point("initialize_node")

    # ì—£ì§€ ì—°ê²°
    graph_builder.add_edge("initialize_node", "perform_search_node") # ì´ˆê¸°í™” í›„ ë°”ë¡œ ê²€ìƒ‰
    graph_builder.add_edge("perform_search_node", "evaluate_snippets_node")
    graph_builder.add_edge("evaluate_snippets_node", "generate_evaluate_plan_node")
    graph_builder.add_edge("generate_evaluate_plan_node", "update_overall_best_node")

    # ì¡°ê±´ë¶€ ì—£ì§€: update_overall_best_node ì´í›„ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •
    graph_builder.add_conditional_edges(
        "update_overall_best_node", # ì´ ë…¸ë“œ ì‹¤í–‰ í›„
        conditional_edge_decide_next_step, # ì´ í•¨ìˆ˜ë¡œ ë‹¤ìŒ ê²½ë¡œë¥¼ ê²°ì •
        {
            "refine_query_node": "refine_query_node",   # ì¿¼ë¦¬ ê°œì„  í•„ìš” ì‹œ
            "prepare_output_node": "prepare_output_node"    # ì¢…ë£Œ ì¡°ê±´ ì¶©ì¡± ì‹œ 
        }
    )

    # refine_query_node ì‹¤í–‰ í›„ ë‹¤ì‹œ ê²€ìƒ‰ ìˆ˜í–‰
    graph_builder.add_edge("refine_query_node", "perform_search_node")

    # ìµœì¢… ì¶œë ¥ ë…¸ë“œ ì´í›„ ê·¸ë˜í”„ ì¢…ë£Œ
    graph_builder.add_edge("prepare_output_node", END)

    # ê·¸ë˜í”„ ì»´íŒŒì¼
    compiled_graph = graph_builder.compile()
    logging.info("LangGraph for complex search has been compiled.")
    return compiled_graph

# ì „ì—­ ê·¸ë˜í”„ ì¸ìŠ¤í„´ìŠ¤ (ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ í•œ ë²ˆ ë¹Œë“œ)
# ì£¼ì˜: ëª¨ë¸/í”„ë¡œì„¸ì„œê°€ ë¡œë“œëœ í›„ì— ë¹Œë“œí•´ì•¼ í•¨.
# Raika_Gemma_FastAPI.pyì˜ startup ì´ë²¤íŠ¸ì—ì„œ set_model_and_processor í˜¸ì¶œ í›„ ë¹Œë“œí•˜ëŠ” ê²ƒì„ ê¶Œì¥.
compiled_complex_search_graph: Optional[StateGraph] = None

def initialize_and_get_compiled_graph():
    global compiled_complex_search_graph
    if compiled_complex_search_graph is None:
        if model and processor:
            compiled_complex_search_graph = build_complex_search_graph()
        else:
           logging.error("Cannot build LangGraph: Model or processor not yet initialized.")
            # ì´ ê²½ìš°, LangGraphë¥¼ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜ëŠ” í˜¸ì¶œë˜ë©´ ì•ˆë¨.
    return compiled_complex_search_graph


def search_and_reason_for_complex_problem_langgraph(
    query: str,
    problem_type: str,
    additional_context: Optional[str] = None,
    max_iterations: int = 2, # ê·¸ë˜í”„ ë£¨í”„ ë°˜ë³µ íšŸìˆ˜
    language="en",
    user_info_uncertain: bool = False
) -> Optional[Dict[str, any]]:
    """
    [LangGraph ì ìš© ë²„ì „]
    ë³µì¡í•œ ë¬¸ì œì— ëŒ€í•´ ê²€ìƒ‰, ê´€ë ¨ì„± í‰ê°€, ì ìš© ê³„íš, ê³„íš í‰ê°€ë¥¼ LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜í–‰.
    
    Args:
        user_info_uncertain: Trueì¼ ê²½ìš°, ì˜¤ë¡œì§€ ì‚¬ìš©ì ì§ˆë¬¸ë§Œ ê¸°ë°˜ìœ¼ë¡œ í‚¤ì›Œë“œ ìƒì„±
    
    Returns: ìµœì¢… ì‘ë‹µ ìƒì„±ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None (ì˜¤ë¥˜ ì‹œ).
    """
    global model, processor # í•¨ìˆ˜ ë‚´ì—ì„œ ì‚¬ìš© ì „ì— í™•ì¸
    if not model or not processor:
        logging.error("LangGraph search_and_reason: Model or processor not set. Cannot proceed.")
        return {
            "status": "error",
            "message": "LLM Model/Processor not initialized.",
            "query": query,
            "language": language
        }
    
    graph_app = initialize_and_get_compiled_graph() # ê·¸ë˜í”„ ê°€ì ¸ì˜¤ê¸° (í•„ìš”ì‹œ ë¹Œë“œ)
    if not graph_app:
        logging.error("LangGraph search_and_reason: Compiled graph is not available.")
        return {
            "status": "error",
            "message": "LangGraph application not compiled or available.",
            "query": query,
            "language": language
        }

    logging.info(f"Starting LangGraph complex search for: '{query}' (Type: {problem_type}, Lang: {language}, MaxIters: {max_iterations}, UserInfoUncertain: {user_info_uncertain})")

    initial_state: ComplexSearchGraphState = {
        "original_query": query,
        "problem_type": problem_type,
        "language": language,
        "max_iterations": max_iterations,
        "additional_context": additional_context,
        "user_info_uncertain": user_info_uncertain,
        # ë‚˜ë¨¸ì§€ í•„ë“œë“¤ì€ initialize_nodeì—ì„œ ì±„ì›Œì§
        "current_search_query": query, # ëª…ì‹œì  ì´ˆê¸°í™”
        "iteration_count": 0,
        "reasoning_log": [],
        "search_results_snippets": [],
        "relevant_snippets_evaluation": [],
        "current_best_snippet_from_iteration": None,
        "current_plan": None,
        "current_plan_evaluation": None,
        "best_overall_snippet_text": None,
        "best_overall_plan_text": None,
        "highest_overall_confidence": -1,
        "final_output_for_raika": None
    }

    try:
        # config={"recursion_limit": max_iterations + 5} ì™€ ê°™ì´ ì¬ê·€ ê¹Šì´ ì„¤ì • ê°€ëŠ¥ (ë£¨í”„ê°€ ìˆëŠ” ê²½ìš°)
        # LangGraphëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ìƒíƒœë¥¼ ì „ë‹¬í•˜ë©° ë…¸ë“œë¥¼ ì‹¤í–‰í•˜ë¯€ë¡œ, ì¼ë°˜ì ì¸ Python ì¬ê·€ ê¹Šì´ì™€ëŠ” ë‹¤ë¦„.
        # max_iterationsëŠ” ê·¸ë˜í”„ ë¡œì§ ë‚´ì—ì„œ ë°˜ë³µì„ ì œì–´í•˜ê¸° ìœ„í•œ ìš©ë„.
        final_graph_state = graph_app.invoke(initial_state, {"recursion_limit": max_iterations * 4 + 10}) # ì¶©ë¶„í•œ ì¬ê·€ ê¹Šì´ ì œê³µ

        if final_graph_state and "final_output_for_raika" in final_graph_state:
            output = final_graph_state["final_output_for_raika"]
            logging.info(f"LangGraph execution completed. Status: {output.get('status') if output else 'N/A'}")
            # ìµœì¢… ë¡œê·¸ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            # full_log = "\n".join(final_graph_state.get("reasoning_log", []))
            # logging.debug(f"Full LangGraph Reasoning Log for query '{query}':\n{full_log}")
            return output
        else:
            logging.error(f"LangGraph execution finished but 'final_output_for_raika' not found in state for query '{query}'. State: {final_graph_state}")
            return {
                "status": "error",
                "message": "LangGraph execution error: Final output missing.",
                "query": query,
                "language": language,
                "reasoning_log_summary": "\n".join(final_graph_state.get("reasoning_log", ["No log available"])[:5]) # ë¡œê·¸ ì¼ë¶€
            }

    except Exception as e:
        import traceback
        logging.error(f"Exception during LangGraph execution for query '{query}': {e}\n{traceback.format_exc()}")
        return {
            "status": "error",
            "message": f"LangGraph invocation exception: {str(e)}",
            "query": query,
            "language": language
        }


if __name__ == "__main__":
    # ë…ë¦½ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ëª¨ë¸ ë¡œë“œ
    model, processor = load_model_for_testing()

    # test_query_en = "What were the key concepts used to prove Fermat's Last Theorem?"
    # print(f"\n--- ë³µì¡í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (í•œêµ­ì–´) ---")
    # final_prompt_en = search_and_reason_for_complex_problem(test_query_en, "complex_math_problem", language="en")

    # # ì˜ì–´ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
    # test_query_en = "Get a quote for a desktop with RTX 5080"
    # prompt_en = process_with_rag(test_query_en, max_context_length=1000, language="en")
    # print(f"Generated prompt for English query '{test_query_en}':")
    # print(prompt_en)
    
    # # í•œêµ­ì–´ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
    # test_query_ko = "RTX 5080 ê·¸ë˜í”½ì¹´ë“œê°€ ì¥ì°©ëœ ë°ìŠ¤í¬íƒ‘ ê²¬ì  ì•Œë ¤ì¤˜"
    # prompt_ko = process_with_rag(test_query_ko, max_context_length=1000, language="ko")
    # print(f"í•œêµ­ì–´ ì¿¼ë¦¬ì— ëŒ€í•œ ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ '{test_query_ko}':")
    # print(prompt_ko)
 
    # # ì˜ì–´ RAG ì‹œìŠ¤í…œ ê²°ê³¼ë¥¼ í†µí•œ ì‘ë‹µ ìƒì„± í…ŒìŠ¤íŠ¸
    # messages_en = [
    #     {
    #         "role": "user",
    #         "content": [
    #             {"type": "text", "text": final_prompt_en}
    #         ]
    #     }
    # ]

    # # ë©”ì‹œì§€ë¥¼ ëª¨ë¸ì— ë§ê²Œ ì²˜ë¦¬
    # inputs_en = processor.apply_chat_template(
    #     messages_en, 
    #     add_generation_prompt=True, 
    #     tokenize=True,
    #     return_dict=True, 
    #     return_tensors="pt"
    # ).to(model.device, dtype=torch.bfloat16)

    # input_len_en = inputs_en["input_ids"].shape[-1]

    # # ëª¨ë¸ ì¶”ë¡  ìˆ˜í–‰
    # with torch.inference_mode():
    #     generation_en = model.generate(
    #         **inputs_en, 
    #         max_new_tokens=1500, 
    #         do_sample=True,
    #         temperature=0.7
    #     )
    #     generation_en = generation_en[0][input_len_en:]

    # # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
    # response_en = processor.decode(generation_en, skip_special_tokens=True)
    
    # print("\nGenerated English response:")
    # print(response_en)
    
    # # í•œêµ­ì–´ RAG ì‹œìŠ¤í…œ ê²°ê³¼ë¥¼ í†µí•œ ì‘ë‹µ ìƒì„± í…ŒìŠ¤íŠ¸
    # messages_ko = [
    #     {
    #         "role": "user",
    #         "content": [
    #             {"type": "text", "text": prompt_ko}
    #         ]
    #     }
    # ]

    # # ë©”ì‹œì§€ë¥¼ ëª¨ë¸ì— ë§ê²Œ ì²˜ë¦¬
    # inputs_ko = processor.apply_chat_template(
    #     messages_ko, 
    #     add_generation_prompt=True, 
    #     tokenize=True,
    #     return_dict=True, 
    #     return_tensors="pt"
    # ).to(model.device, dtype=torch.bfloat16)

    # input_len_ko = inputs_ko["input_ids"].shape[-1]

    # # ëª¨ë¸ ì¶”ë¡  ìˆ˜í–‰
    # with torch.inference_mode():
    #     generation_ko = model.generate(
    #         **inputs_ko, 
    #         max_new_tokens=500, 
    #         do_sample=True,
    #         temperature=0.7
    #     )
    #     generation_ko = generation_ko[0][input_len_ko:]

    # # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
    # response_ko = processor.decode(generation_ko, skip_special_tokens=True)
    
    # print("\nìƒì„±ëœ í•œêµ­ì–´ ì‘ë‹µ:")
    # print(response_ko)


# --- (25.05.15) ê°œì„ ëœ RAGë¥¼ ìœ„í•œ ë©”ì¸ í•¨ìˆ˜ ---

    print("\n--- GoogleSearch_Gemma.py: Test Suite ---")

    # # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: ë‹¨ìˆœ ì •ë³´ ê²€ìƒ‰ (ì˜ì–´)
    # test_query_simple_en = "What is the capital of France?"
    # print(f"\n[Test Case 1: Simple Info Retrieval - EN] Query: '{test_query_simple_en}'")
    # search_type_1 = classify_search_type_langchain(test_query_simple_en, language="en")
    # print(f"  - Classified Search Type: {search_type_1}")
    # if model and processor: # ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆì„ ê²½ìš°ì—ë§Œ ì‹¤í–‰
    #     # recursive_searchëŠ” (str, bool, int) íŠœí”Œì„ ë°˜í™˜
    #     rag_content_1, satisfied_1, iterations_1 = recursive_search(test_query_simple_en, language="en", max_iterations=1)
    #     print(f"  - Recursive Search Satisfied: {satisfied_1}, Iterations: {iterations_1}")
    #     print(f"  - RAG Content (first 200 chars): {rag_content_1[:200] if rag_content_1 else 'N/A'}")
    # else:
    #     print("  - Skipping RAG test as model/processor not fully loaded.")

    # # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: ë‹¨ìˆœ ì •ë³´ ê²€ìƒ‰ (í•œêµ­ì–´)
    # test_query_simple_ko = "RTX 5080 ê·¸ë˜í”½ì¹´ë“œê°€ ì¥ì°©ëœ ë°ìŠ¤í¬íƒ‘ ê²¬ì  ì•Œë ¤ì¤˜."
    # print(f"\n[Test Case 2: Simple Info Retrieval (Movie Search) - KO] Query: '{test_query_simple_ko}'")
    # search_type_2 = classify_search_type_langchain(test_query_simple_ko, language="ko")
    # print(f"  - Classified Search Type: {search_type_2}")
    # if model and processor:
    #     rag_content_2, satisfied_2, iterations_2 = recursive_search(test_query_simple_ko, language="ko", max_iterations=2)
    #     print(f"  - Recursive Search Satisfied: {satisfied_2}, Iterations: {iterations_2}")
    #     print(f"  - RAG Content (first 200 chars): {rag_content_2[:200] if rag_content_2 else 'N/A'}")
    # else:
    #     print("  - Skipping RAG test as model/processor not fully loaded.")


    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3: ë³µì¡í•œ ë¬¸ì œ í•´ê²° ê²€ìƒ‰ (ìˆ˜í•™)
    # test_query_complex_math = "í˜ë¥´ë§ˆì˜ ë§ˆì§€ë§‰ ì •ë¦¬ë¥¼ ì¦ëª…í•˜ëŠ” ë° ì‚¬ìš©ëœ í•µì‹¬ ê°œë…ë“¤ì€ ë¬´ì—‡ì¸ê°€?"
    # # test_query_complex_math = "explain the RSA algorithm steps with a simple example"
    # print(f"\n[Test Case 3: Complex Math Problem - KO] Query: '{test_query_complex_math}'")
    # search_type_3 = classify_search_type_langchain(test_query_complex_math, language="ko")
    # print(f"  - Classified Search Type: {search_type_3}")
    # if model and processor:
    #     complex_search_result_3 = search_and_reason_for_complex_problem(test_query_complex_math, search_type_3, language="ko", max_iterations=1)
    #     print(f"  - Complex Search Result Status: {complex_search_result_3.get('status') if complex_search_result_3 else 'N/A'}")
    #     if complex_search_result_3 and complex_search_result_3.get('status') == 'success':
    #         print(f"    - Best Snippet (first 100): {complex_search_result_3.get('best_snippet', '')[:100]}")
    #         print(f"    - Best Plan (first 100): {complex_search_result_3.get('best_plan', '')[:100]}")
    #         print(f"    - Confidence: {complex_search_result_3.get('confidence')}")
    #     elif complex_search_result_3:
    #          print(f"    - Message: {complex_search_result_3.get('message')}")
    #     # print(f"  - Reasoning Summary (first 300 chars): {complex_search_result_3.get('reasoning_summary', '')[:300] if complex_search_result_3 else 'N/A'}")
    # else:
    #     print("  - Skipping Complex Search test as model/processor not fully loaded.")

    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3.1: ë³µì¡í•œ ë¬¸ì œ í•´ê²° ê²€ìƒ‰ (ìˆ˜í•™) (LangGraph)

    # LangGraph ë¹Œë“œ (ëª¨ë¸/í”„ë¡œì„¸ì„œ ì„¤ì • í›„)
    graph_app_instance = initialize_and_get_compiled_graph()

    if graph_app_instance:
        print("\n--- LangGraph ë³µí•© ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (í•œêµ­ì–´) ---")
        # test_query_complex_ko = "ì„œìš¸ì—ì„œ ì œì£¼ë„ê¹Œì§€ ê°€ì¥ ë¹ ë¥´ê²Œ ê°€ëŠ” ë°©ë²•ê³¼ ë¹„ìš©ì€ ì–¼ë§ˆì¸ê°€ìš”? ë ŒíŠ¸ì¹´ í¬í•¨í•´ì„œ ì•Œë ¤ì£¼ì„¸ìš”."
        test_query_complex_ko = "í˜ë¥´ë§ˆì˜ ë§ˆì§€ë§‰ ì •ë¦¬ë¥¼ ì¦ëª…í•˜ëŠ” ë° ì‚¬ìš©ëœ í•µì‹¬ ê°œë…ë“¤ì€ ë¬´ì—‡ì¸ê°€?"
        problem_type_ko = classify_search_type_langchain(test_query_complex_ko, language="ko") # "complex_reasoning_problem" ë˜ëŠ” "complex_math_problem"
        
        print(f"í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: '{test_query_complex_ko}', ë¶„ë¥˜ëœ ìœ í˜•: {problem_type_ko}")

        final_result_ko = search_and_reason_for_complex_problem_langgraph(
            query=test_query_complex_ko,
            problem_type=problem_type_ko,
            max_iterations=2, # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë°˜ë³µ ì¤„ì„
            language="ko"
        )

        print("\n--- LangGraph ì‹¤í–‰ ê²°ê³¼ ---")
        if final_result_ko:
            print(f"ìƒíƒœ: {final_result_ko.get('status')}")
            print(f"ì›ë³¸ ì¿¼ë¦¬: {final_result_ko.get('query')}")
            if final_result_ko.get('status') == 'success':
                print(f"ìµœê³  ìŠ¤ë‹ˆí« (ì¼ë¶€): {final_result_ko.get('best_snippet', '')[:200]}...")
                print(f"ìµœê³  ê³„íš (ì¼ë¶€): {final_result_ko.get('best_plan', '')[:200]}...")
                print(f"ì‹ ë¢°ë„: {final_result_ko.get('confidence')}")
            else:
                print(f"ë©”ì‹œì§€: {final_result_ko.get('message')}")
                print(f"ë‹¬ì„±ëœ ìµœê³  ì‹ ë¢°ë„: {final_result_ko.get('highest_confidence_achieved')}")
            
            # ì „ì²´ ì¶”ë¡  ë¡œê·¸ (ë„ˆë¬´ ê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¼ë¶€ë§Œ ì¶œë ¥ ë˜ëŠ” íŒŒì¼ ì €ì¥ ê³ ë ¤)
            # print("\n--- ì „ì²´ ì¶”ë¡  ë¡œê·¸ ---")
            # reasoning_log_summary = final_result_ko.get('reasoning_summary', "ì¶”ë¡  ë¡œê·¸ ì—†ìŒ.")
            # print(reasoning_log_summary[:1000] + "..." if len(reasoning_log_summary) > 1000 else reasoning_log_summary)
        else:
            print("LangGraph ì‹¤í–‰ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ ë˜ëŠ” ê²°ê³¼ ì—†ìŒ.")
    else:
        print("LangGraph ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë¹Œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸/í”„ë¡œì„¸ì„œ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")

    print("\n--- GoogleSearch_Gemma.py: LangGraph í…ŒìŠ¤íŠ¸ ì™„ë£Œ ---")


    # # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 4: í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤ (RAG_Result.txtì˜ ë‚´ìš© ì¬í˜„)
    # # ì´ í…ŒìŠ¤íŠ¸ëŠ” Raika_Gemma_FastAPI.pyì˜ handle_general_conversation ë‚´ì—ì„œ í…ŒìŠ¤íŠ¸í•˜ëŠ” ê²ƒì´ ë” ì í•©í•©ë‹ˆë‹¤.
    # # GoogleSearch_Gemma.pyëŠ” í‚¤ì›Œë“œë¥¼ ì…ë ¥ë°›ëŠ” ê²ƒì„ ì „ì œë¡œ í•˜ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ì§ì ‘ì ì¸ ì¬í˜„ì´ ì–´ë µìŠµë‹ˆë‹¤.
    # # ë‹¤ë§Œ, ë¹ˆ ê²€ìƒ‰ì–´ë¡œ recursive_searchë¥¼ í˜¸ì¶œí–ˆì„ ë•Œì˜ ë™ì‘ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # test_query_empty_keywords = "" # assess_search_requirementê°€ í‚¤ì›Œë“œ ì¶”ì¶œì— ì‹¤íŒ¨í•œ ìƒí™© ê°€ì •
    # print(f"\n[Test Case 4: Empty Keywords Scenario] Query: '{test_query_empty_keywords}' (Simulating keyword extraction failure)")
    # search_type_4 = classify_search_type_langchain(test_query_empty_keywords, language="en") # "simple_information_retrieval" ë°˜í™˜ ì˜ˆìƒ
    # print(f"  - Classified Search Type: {search_type_4}")
    # if model and processor:
    #     # recursive_searchëŠ” ê²€ìƒ‰ì–´ê°€ ë¹„ì–´ìˆìœ¼ë©´ Google Searchì—ì„œ ë¹ˆ ê²°ê³¼ë¥¼ ë°›ê³ , ì´ë¥¼ ì²˜ë¦¬í•´ì•¼ í•¨
    #     rag_content_4, satisfied_4, iterations_4 = recursive_search(test_query_empty_keywords, language="en", max_iterations=1)
    #     print(f"  - Recursive Search Satisfied: {satisfied_4}, Iterations: {iterations_4}")
    #     print(f"  - RAG Content: {rag_content_4 if rag_content_4 else 'N/A'}") # "No search results found..." ì˜ˆìƒ
    # else:
    #     print("  - Skipping RAG test as model/processor not fully loaded.")

    # # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 5: Raikaê°€ ì˜í™”ë¥¼ ì°¾ì•„ë‹¬ë¼ëŠ” ìš”ì²­ (RAG_Result.txt)
    # # Renard: *Pet your head smoothly* I had a dream that I was watching a movie I saw a long time ago. Raika, I'm looking for a this movie. ğŸï¸ Can you help me? ğŸ¤”
    # # Raikaì˜ handle_general_conversationì—ì„œ assess_search_requirementë¥¼ í˜¸ì¶œí•˜ê³ , ê·¸ ê²°ê³¼ë¡œ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ìƒì„±í•´ì•¼ í•¨.
    # # ì—¬ê¸°ì„œëŠ” assess_search_requirementê°€ "old movie dream" ê°™ì€ í‚¤ì›Œë“œë¥¼ ìƒì„±í–ˆë‹¤ê³  ê°€ì •í•˜ê³  í…ŒìŠ¤íŠ¸.
    # user_input_movie = "I had a dream that I was watching a movie I saw a long time ago. Raika, I'm looking for a this movie."
    # # ê°€ì •ëœ í‚¤ì›Œë“œ (ì›ë˜ëŠ” assess_search_requirementê°€ ìƒì„±)
    # assumed_keywords_for_movie = "old movie dream" # ë˜ëŠ” "movie long time ago dream" ë“±
    # print(f"\n[Test Case 5: Movie Search from RAG_Result.txt - User: '{user_input_movie}']")
    # print(f"  - Assumed Keywords (from assess_search_requirement): '{assumed_keywords_for_movie}'")
    # search_type_5 = classify_search_type_langchain(assumed_keywords_for_movie, language="en")
    # print(f"  - Classified Search Type: {search_type_5}")
    # if model and processor:
    #     rag_content_5, satisfied_5, iterations_5 = recursive_search(assumed_keywords_for_movie, user_input_movie, language="en", max_iterations=1)
    #     print(f"  - Recursive Search Satisfied: {satisfied_5}, Iterations: {iterations_5}")
    #     print(f"  - RAG Content (first 200 chars): {rag_content_5[:200] if rag_content_5 else 'N/A'}")

    #     # ë§Œì•½ RAG_Result.txt ì²˜ëŸ¼ Iter 1: No content found... ê°€ ë°˜ë³µëœë‹¤ë©´,
    #     # recursive_search ë‚´ë¶€ì˜ Google Search_keywords)ê°€ ë¹ˆ ê²°ê³¼ë¥¼ ë°˜í™˜í–ˆê±°ë‚˜,
    #     # ê·¸ ì´í›„ combined_resultsê°€ ë¹„ì–´ìˆì–´ì„œ LLM í‰ê°€ ë‹¨ê³„ë¡œ ë„˜ì–´ê°€ì§€ ëª»í•˜ê³  ë‹¤ìŒ iterationìœ¼ë¡œ ê°„ ê²ƒì…ë‹ˆë‹¤.
    #     # generate_search_keywordsê°€ ë¹ˆ í‚¤ì›Œë“œë¥¼ ë°˜í™˜í•˜ê±°ë‚˜, Google Search ìì²´ê°€ ê²°ê³¼ë¥¼ ëª» ì°¾ëŠ” ê²½ìš° ë°œìƒ.
    #     # RAG_Result.txtì˜ "Iter 1: No content found from any individual keyword searches in this iteration."ëŠ” FastAPI ìª½ì˜ ë£¨í”„ì—ì„œ ë°œìƒ.
    #     # GoogleSearch_Gemma.pyì—ì„œëŠ” recursive_searchê°€ ë¹ˆ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í˜•íƒœë¡œ ë‚˜íƒ€ë‚  ê²ƒì…ë‹ˆë‹¤.
    #     if not rag_content_5 or "No search results found" in rag_content_5:
    #          print(f"  - !!! Simulating RAG_Result.txt issue: recursive_search returned no meaningful content for keywords '{assumed_keywords_for_movie}'. This would lead to 'No content found' in FastAPI.")
    # else:
    #     print("  - Skipping RAG test as model/processor not fully loaded.")


    print("\n--- Test Suite Finished ---")